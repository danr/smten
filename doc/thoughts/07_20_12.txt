
Fri Jul 20 09:10:33 EDT 2012

Goal today: improve performance of the elaborator.

Immediate question: why are we performing lookupVarInfo so many more times now
than before simplifying the elaborator? I know I'm doing a lot more work than
I have to. I should understand this before trying to speed up the
lookupVarInfos.

Side note: the yices queries I was running finished.

yices2 - less than 1 second
yices1, using yices2 query: 120 minutes
yices1, using yices1 query: 266 minutes

So, this suggests a future plan: ditch having a separate yices1 and yices2
target and compiler. Just use the yices2, and have a way to dump the same
syntax and do FFI to both yices1 and yices2.

Okay then. Let's get started with the elaborator issue.

Summary:
ytc: Changed yices to use term constructor FFI.
17 seconds to run the query, dominated by "rules". So I figured I'd simplify
the elaborator. I simplified a lot, and it started going slower.

A big difference is the number of times lookupVar is called.

ytc:     150065 calls to lookupVar from the varredR rule.
selab: 10250938 calls to lookupVar from elaborate. They should be the same.

Why does selab have 2 orders of magnitude more? I'm almost certain this is the
cause of performance drop from ytc to selab.

What could it be?

We are running a query. Everything is fully inlined and elaborated...

It's perhaps interesting that the cause of this is in runQuery.elaborated, not
in the yices compiler...

Elaborate in selib is called 24283315 times. In ytc... it's only called 4778
times. That's rather surprising... Oh. Not really, because we call it
recursively. The runQuery is called 29 times in ytc, same as selib.

Perhaps I ought to add more profiling sites. Try to identify when each
specific elaborate rule fires.

The other thing to do is look at the order in which rules are firing, and make
sure they are the same? Let's see...

Fri Jul 20 09:35:17 EDT 2012

The order looks the same to me.

Hmm... it would seem that we spend a lot of time looking up vars which aren't
there! That's surprising to me. If we didn't waste time looking these up, we
would end up doing less lookups.

I wonder if simplifying inside lambdas and case matches is leading to this? I
don't think it can, because I check for an empty environment... But it's worth
a try getting rid of that.

No. That causes other problems.

How about asking what it is we are looking up and not finding.
Is it primitives?

It's free~1, free~2, and the primitive add and eq operators mostly.
Now... why would we ever encounter primitive add and eq for var lookup? Isn't
that only if we aren't fully applied?

Let me see if ytc is similar.

ytc is somewhat similar, but free~1 and free~2 and + happen 5 to 10 times more
in selab.

I should be able to make a trace log of what's going on in each, right? Every
time we have a reduction, say what the reduction was... It may force us to
evaluate things we wouldn't otherwise when printing unfortunately... But in
the very least it would tell us what order things are being done in. And who
knows, forcing early evaluation may lead to some interesting observations.

Let me try to do this, and try to do it in such a way that the logs are
comparable.

Fri Jul 20 10:38:24 EDT 2012

I feel like, if I could trace through the work that was being done, I could
understand this more easily. The trouble is... observing things messes with
laziness, which is, I think, a very important factor here.

What if I printed out the core operations.
So, for example:
 case match failed
 case match succeeded
 prim +-*<>==
 reduce name in lam
 lookup var
 
Do that in both selab and ytc, and I should be able to see the order in which
things happen, and maybe follow the expression by hand (though that could be
ugly). Perhaps on much simpler cases. That would be good. Find a simpler case
I can better understand which shows the same performance issues. Step through,
and really get a grasp of what's going on, and how they differ.

Good. I like this plan. Let me try it out.

Fri Jul 20 11:14:14 EDT 2012

You know, I really think the issue may be that elaborate does simplification
inside lambdas. What's the problem with that? We end up inlining variables
which we haven't looked up. It forces us to do the lambda reduction before the
inlining, which is exactly what we don't want.

So, I feel like what I need is to really separate the notion of simplify and
elaborate. Elaborate should just elaborate, do as little work as it can to get
to the head form. Don't simplify inside cases and lets. Simplify should
simplify as much as possible... Hmm... Perhaps the issue is that simplify
doesn't necessarily simplify.

Consider the following:

\a -> (\b -> b + b) foo

Elaborate would not reduce it any further.
With simplify, though, it turns into:

\a -> foo + foo

Now, if foo is big and complicated, we have to evaluate it twice! We've lost
our sharing.

Aha. So I bet this is the issue. We shouldn't call simplify unless we really
mean it... We shouldn't call simplify unless we know there isn't anything in
the environment.

Or rather... we shouldn't simplify unless we know we can't make further
progress on the given names from the environment.

What's the reason we want to simplify in the first place?

It's so we can do things like:

(\_ -> error~) "foo"

Becomes (error~).

Which thus removes "foo", and removes a requirement on lists.

Okay. Here's the plan. Split simplify and elaborate into two parts.

Elaborate is the same, only it doesn't go inside matches or lambdas.
Simplify calls elaborate to get a head start, then goes inside lambdas and
matches. Simple. I bet this is the issue.

Fri Jul 20 11:44:17 EDT 2012

Yup. That was it. Cool. Now the simplified elaborator makes more sense.

Just one thing that's annoying: we have a lot of code duplicated. I should
have a common function for elaborate and simplify. Let's say it takes a maybe
environment: if Just, elaborate. if Nothing, simplify.

Or... just check for an empty environment. That seems reasonable to me.

Good. That works how I like. Let me clean up all this stuff now.

Fri Jul 20 11:57:26 EDT 2012

It's all cleaned up. Let's see if this helps Myron out any.
I still haven't resolved the yices1 issue. Let me send an email about that.
And I think I should simplify the yices target and SMT to use the same Yices
kind of thing. Use the same simple syntax (yices2) and same target generation.
That should save me a bunch of work and will, from what I've seen, perhaps
make things go even faster.

All you give up is lambdas which can't be inlined in yices1, but if they can't
be inlined, yices1 can't handle them anyway.

Fri Jul 20 12:49:18 EDT 2012

What's next? I'm not sure what my priorities should be.

Let me claim the yices2 performance is good enough, including the whole Seri
elaboration process, until suggested otherwise.

We have the yices1 performance problem to try and resolve. I have some ideas
of what about Myron's direct implementation is different from my
implementation that could be causing the blow up:
- Assume no errors
- Inline nothing (perhaps yices1 takes advantage of more info that way?)
- Introduce primitive structural equality instead of (==)

Other ways to make progress with that?
- send the query to Bruno, see if he has any insight
- read over Myron's query, look at how else it is different from what I
  generate.

It would be good if I can get a sense of what the issue is. Maybe, if all
these things are easy to do, I should try them out and just see what happens.
See if anything makes a major difference. That could take time, to see how
long the queries take.

Aside from that performance issue, there's some cleanup I'd like to do:
+ merge the two yices targets into one.
+ fix the parser and pretty printer problems (add a test case which pretty
  prints, parses, and compares the result)
+ Fill out the Prelude with stuff Myron had to add for his query.
+ Check if yices2 seg faulting went away since ytc (should be easy to check).
+ Define Query as opaque type.
- Deriving Eq
- Move Infer, Check, and Solve into subdirectory of Type. Maybe move Types
  into Prelude.

And then there are the big things:
- Modularity
- Numeric types.

Look, it's Friday. No immediate pressures. I should take this time to clean up
as much as I can. When I get bored with cleanup, go back to thinking about
numeric types and modularity.

Fri Jul 20 13:03:13 EDT 2012

You know what? I think the ytc change fixed the yices2 seg faulting. I bet it
was something to do with allocating large strings. That's really cool. Let me
assume that's solved until I hear otherwise.

Fri Jul 20 13:25:02 EDT 2012

How to test the pretty printer and parser?

Well, I should think I should print and parse and see what happens. The
problem is, modularity and builtins and such makes that not practical. That
is, we print post flattened and want to parse pre flattened? It won't
work.

Having the ability to parse back in has been helpful though. Certainly pretty
printing itself is helpful, as is parsing... I just can't seem to put them
together.

I mean, I can fix the pretty printer and parser bugs, but without a way to
test them... That's annoying.

I suppose just do manual testing for pretty printing now. The parse is easy to
test.

Fri Jul 20 13:50:18 EDT 2012

I think I want to merge the yices now. I think I know of a way we can do it,
keeping as much code in common as possible, while still maintaining the
ability to differentiate later on if desired.

Here's the plan. Parameterize the Yices syntax based on whether it's yices1 or
yices2? Maybe that's overkill.

How I want it to work is...

I have a syntax, which is my yices2 syntax, which I use for everything.
(Means we can't have lambdas?)

Oh... I want my pretty printer to take an argument: the yices version to use.
The only difference is, whatever the difference is. It will be nice to have my
own pretty printer for yices1 anyways, which doesn't use super long lines.

An FFI for yices1. An FFI for yices2. Both using the syntax, ideally with the
same interface.

I can make them have the same interface. Restrict and generalize where needed.

Err... it would be awesome if we had two different types somehow. A Yices1
type, and a Yices2 type. Then use typeclasses to distinguish between them. But
that doesn't work so well here, does it.

Certainly the context is different. So something is different.

I think...

Have a Yices class:

class Yices a where
    version :: a -> Integer
    mkYices :: IO a
    run :: a -> Command -> IO ()
    check :: a -> IO Result
    getIntegerValue :: a -> String -> IO Integer

That's not too bad.

Now... pretty print takes a Command. But Command isn't parameterized by type,
so how does it know which to use?

The other idea is to annotate the syntax elements.

Or just pass a yices version to everything.

How about this. Have something called:

YicesVersion = Yices1 | Yices2.

Keep Command and friends as is.
Implement pretty :: (Ppr a) => YicesVersion -> a -> String
Compilation takes the yices version.

Have a class:

YicesFFI a where
Same as above class, only version returns the version.

runYices takes a version. It's a wrapper around something which calls mkYices
with an explicit type sig depending on the version, and continues that way.

Good! Sounds like a plan. Let me try it out. I'll use all the Yices2 stuff as
a starting point.

Um... this could use its own branch.

Fri Jul 20 14:22:39 EDT 2012

Trouble. I really am restricting the yices2 command set a bunch if I do that.
So my syntax will not be useful in general as a yices2 syntax or a yices1
syntax. It will mostly just be useful for the Seri, which tries to use both
Yices1 and Yices2. Is that okay?

Sure. I suppose I can extend it later if desired.

Fri Jul 20 14:54:04 EDT 2012

I'm concerned that I can't have Yices1 and Yices2 at the same time, because
the libraries conflict. This means, while I can have common code, I should
make sure runYices1 and runYices2 are accessed from different modules.

Fri Jul 20 15:26:56 EDT 2012

There! It works. Yices1 and Yices2 are now merged.

I think... it's good for me not to be duplicating code, and to take advantages
of improvements. I worry it's maybe a little bit of glombing things together
which need not be glombed together... oh well. I can always change back if I
want.

Fri Jul 20 15:32:27 EDT 2012

Nirav says the yices1 performance thing isn't so important to get stuck
wasting time on. Good.

What do you think about adding sugar for deriving Eq? That would be very nice
to have, and not so much work, I don't think.

Let me try that, then finish up with some thoughts on the next step for
modularity and more thoughts on numeric types.

Fri Jul 20 15:56:51 EDT 2012

Ug, so I'm not in the mood to work out the types and such for deriving Eq.
It's not hard. Just a bit tedious. I can come back to this later.

Let me finish with some thoughts on modularity and numeric types.

Fri Jul 20 15:59:19 EDT 2012

Modularity.

I think with the current version, it's possible to have two things with the
same name in the same system, as long as the modules which define them aren't
both imported in the same module that uses one of them. That's good. I haven't
tested this, but I think that's what should happen.

The next thing we really want, I think, is to have an explicit export list for
values. Hmm... it would seem I'm not much in the mood for this either right
now. I wonder if there is any low hanging fruit performance wise now. It
shouldn't take 4 seconds to do Myron's query. It should be, like
instantaneous, no?

hum. Or maybe I should work on numeric types offline.

There are many questions with numeric types. One thing you want to do with
numeric types is get their value as an integer.

This can be done using typeclasses, but to me it feels really cludgy, because
you end up specifying "undefined" as an argument all over the place. Is there
a neater way to do this?

The real question is, say I have a type Bit n. Given an arbitrary type which
could contain a numeric type, I'd like to figure out what that numeric type
is. Or, maybe given an object which contains a numeric type.

I suppose we could have numeric types have objects. Each one has a
constructor. Then say Bit n carries around that object... But that's annoying.

Maybe the real question is, how would you implement sizeof? You should be able
to do it entirely within the language, I think.

sizeof :: Bit n -> Integer

If bit was defined as...

data Bit n = Numeric n => Bit n [Bool]

Then we could say:

sizeof :: Bit n -> Integer
sizeof (Bit l _) = valueof l

The haskell way I would do would be something like:

sizeof :: Bit n -> Integer
sizeof b =
  let ton :: Bit n -> n
      ton _ = undefined
  in valueof (ton b)

It's not entirely unreasonable, but it is, in my mind, a bit of a hack. And we
don't support type signatures in let bindings in seri at this point, so you'd
have to pull the function outside to the top level.

I could have a special syntax for it. Let's say valueof is a keyword. The
syntax could be...

valueof (Type -> Type) Exp

For example:
    valueof (Bit n -> n) b
Says, get the value of the numeric type produced when given the given type.
Constraints are: The expression must have the specified input type. The
specified output type must be a numeric type.

This is like a pattern match on types. Maybe we could just allow a pattern
match on types like thing? Only, I feel like it would only be useful with
valueof, so why not make valueof special?

Of course, since the result is always expected to be as single type, we could
have pattern matching.
    valueof (Bit @) b

If you had two numeric types, you might say...
    valueof (Range @ _) r 
    valueof (Range _ @) r

That sort of thing.

I think we should have a Numeric class built in?
Are numeric types always in their own space? Or does it make sense for VarT to
work on them? For example, I would think it doesn't make any sense to have 
[8] as a valid type. That's interesting. So then maybe we could really make a
special place for numeric types and restrict them to that special place?

For example, I think we only ever use numeric types as a parameter to a data
type. We certainly don't ever construct or deconstruct objects of that type.

So let's say I have some syntax...
In a data declaration, say a variable preceded with # is a numeric parameter.

data Bit #n = ...

We could have multiple...

data BitPair #n #m = BitPair (Bit#n) (Bit#m)

We can specify functions like:

concat :: (m+n ~ k) => Bit#m -> Bit#n -> Bit#k

We can get the value of something:

sizeof :: Bit #n -> Integer
sizeof b = valueof (Bit #) b

Hmm...
sizeof = valueof (Bit #)

It's almost like valueof is some sort of sugar...

Perhaps we should just have a sugar for type declarations?

No...

Maybe there's an idea that a numeric type can't stand on its own except in
numeric type constraints. So: Bit#n -> n would not be allowed. Or, I guess,
the n and #n could have different name spaces. Of course, this is trivial to
get around using a unit type:

data Unit#n = Unit

Which you could then use instead. So that seems a silly restriction.

Or! We could automatically generate numeric type functions for each numeric
type parameter in our data declarations.

data Bit#(n :: sizeof) = ...

Declares both Bit, and declares a function of type:

sizeof :: Bit#n -> Integer

And provide the primitive valueof :: #n -> Integer?
But no, we don't need that. We can give some prelude:

data NumT #valueof = NumT

And now, type numerals can be syntactic sugar for this NumT.

Of course, if you have just this, you don't need valueof, because you can pass
the numeric object around.

Okay, so it sounds to me like what we want is:

valueof :: #n -> Integer

As a primitive.
And you can construct an instance of a specific numeric type... um... with a
numeric type data constructor: #8, #1, etc...

(valueof #8) == 8

sizeof (Bit n _) = valueof n

Should valueof be strict in its argument? I suppose it would be nice if it
weren't. Add more flexibility.

Hmm... So it would seem I'm coming to settle somewhere.

Numeric type constructor syntax:
    #0, #1, #2, ...
    NumT 0, NumT 1, NumT 2, ...
    
Numeric type data constructor syntax
    #0, #1, #2, ...
    ConE "#0", ConE "#1", ConE "#2", ...

Numeric type constraint on variable type:
    #n ...

Why do we need that. Why not declare a builtin class?

Perhaps the builtin class is called #.

concat :: (#m, #n, #k) => Bit m -> Bit n -> Bit k

Except, that would let us construct a Bit Integer, for example, which makes no
sense. So, unless I want to have contexts in data type declarations, it may
make sense to have the special syntax for this. And we want to do kind
checking somewhere.

Bit has kind: # -> *
m has kind whatever.
#m has kind #

So we treat numeric type as a different kind of type? That makes sense to me.

concat :: Bit m -> Bit n -> Bit k
concat :: Bit m -> Bit n -> Bit (m+n)
concat :: Bit #m -> Bit #n -> Bit #(m+n)
concat :: (m+n = k) => Bit m -> Bit n -> Bit k

Okay, how about this as a proposal:

Abstract syntax:
    Type = ...
         | NumT Integers            -- ^ #0, #1, #2, ...
         | NumOpT NumOp Type Type   -- ^ <type> <op> <type>

    NumOpT = AddNT  -- ^ +
             MulNT  -- ^ *
             SubNT  -- ^ -
             DivNT  -- ^ /
             MinNT  -- ^ ∧ 
             MaxNT  -- ^ ∨ 
             LogNT  -- ^ //
             ExpNT  -- ^ **

    Class = ...
          | NumCls NumClsOp Type Type

    NumClsOp = EqNCO   -- ^ =
               LtNCO   -- ^ <
               GtNCO   -- ^ >
               etc...

The thing I hate is, we now have so much specific stuff hard wired into the
IR. It just makes things so complicated.

Like, literally, the syntax for numeric types alone is as big as the whole
rest of the syntax.

Oh well. I'll think about things. Perhaps we could have a numeric type prelude
like thing and use names instead of symbols, and just don't interpret any of
the symbols we don't know about.

