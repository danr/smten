
Thu Aug  2 08:37:00 EDT 2012

I had more thoughts. I don't think this heap elaborator will solve all my
problems. I think I need to work out my problems first, then there will be a
path to switch to the heap elaborator later to improve performance.

What am I thinking?

The heap elaborator works best if you can give it an expression to elaborate
and have it elaborate the whole thing. It doesn't do so well for repeated
elaboration of parts of the same expression, because you have to rebuild the
heap over and over again. Either that, or you have to expose the internals of
the heap to the user, which is very not nice.

We can make good use of it, however, if we give it as an expression to
elaborate the predicate to assertions.

So, it makes sense to have a simple elaborator which really is as lazy as can
be. This we can use to drive the query monad, which for now is like trivial to
elaborate, because we aren't doing any fancy stuff to figure out what the next
questions to ask are.

I suspect the heap elaborator will do a much better job capturing sharing then
my hacks to the full elaborator. The heap elaborator could also be used in
serie if desired, but it's good to have the simple elaborator around still.

Perhaps the heap elaborator could be made lazy, so it doesn't build a heap for
parts of an expression until you ask to elaborate them... That might be a way
to replace the simple elaborator with the heap elaborator while maintaining
both performance and the proper abstraction when using it in an interactive
way like we do to run queries.

In summary: heap elaborator has great potential, but ultimately we still need
a simple elaboration which is lazy enough not to simplify assertion predicates
any. So I should focus on getting that work, before the heap elaborator.

What are the current problems facing this solution?
- The Array SMT test hangs, no idea why.
- The BCL3 SMT test has an error, because a lambda term shows up.

The two problems, I suspect, are completely unrelated, but should both be
understood.

To understand the array problem: run the array test with profiling turned on.
Let it run for a good while, then look at the profile to see where it's stuck.
Narrow things down from there.

To understand the bcl3 problem: look at the error message, figure out the case
I'm not covering of separated lambdas from arguments.

That's the plan. Let me get started.

Thu Aug  2 08:53:59 EDT 2012

The array test is running. I'll give it a while.

The BCL3 problem is the following case:

let f =  (let a = ... in \x -> ...)
in f x
   
We did all that work to push the lambdas in, and now...

Options: beta reduce any expression which contains a LamE anywhere in it.
Err... that's not going to work. We'll end up getting rid of any sharing.

It looks like, in this case, you want to lift the lambda out. It's like the
opposite direction as before :(.

You know what? I'm thinking the pursuit of this sharing elaborator is not
worth my time. Better to go to the heap elaborator, which we want to do
eventually, should be higher performance, easier to implement, and rather more
effective in capturing sharing.

This does not really go against what I proposed just before... well, sort of,
but it was a hole in what I said. Which is... the problem with bcl3 is a bug
in the sharing implementation, not a bug in the lazy simplification
implementation.

What I should really do, before going on to implement the heap elaborator, is
make sure the expressions the yices target is getting to simplify are fully
un-elaborated.

Thu Aug  2 09:00:29 EDT 2012

So, the array test conked out. Looks like an infinite loop doing:

elaborate
 alpharename
 deleteall

Perhaps I can add SCCs to figure out which annotation is the problem.

Meanwhile, I had another idea for how to fix the sharing in the full
elaborator. Beta-reduce any arg which has a function type, rather than
just top level lambdas. That should hopefully get what I want.

Thu Aug  2 09:14:49 EDT 2012

Well, that fixed the BCL3 problem. But it doesn't seem as if the performance
has improved any. Let's look again at the size of the query.

Thu Aug  2 09:21:20 EDT 2012

Well, so yes, BCL3 has sharing implemented now. It still takes yices the same
(small) amount of time to execute it with just 1 question. It's hard to say
about performance. But we only cut the query in half, from 9M down to 5M. Not
quite as much sharing as I would have liked, that's for sure.

Perhaps the heap elaborator can do much much better. Actually, I suspect it
probably can do significantly better, because it shares things involving
lambdas instead of duplicating them.

Let me also check that the yices target is getting the nicest arguments it can
to elaborate.

Thu Aug  2 09:28:54 EDT 2012

It's close. The problem is, we do beta reduction for things not involving free
variables, and those arguments get elaborated before being reduced, which
means inlining of variables.

Better to be lazy? Like, what if I don't elaborate before being reduced?

Yet again, it seems like a heap elaborator could help us here. Let me try
that. Meanwhile... more info on the array case?

It's beta reduction getting array stopped. Let me try not to do silly things
and somehow force progress.

I'm afraid we could be getting into some loop of beta reduce / push in / beta
reduce / push in, or something like that.

Thu Aug  2 09:34:51 EDT 2012

Okay, I see how something could lead to an infinite loop. Trying to fix...

Thu Aug  2 09:37:21 EDT 2012

Looks like I really can't not elaborate before subsitution in the elaborator,
otherwise things will take forever.

We still have the Array bug.

Here's what I want to do:
1. Fix the array bug.
Now everything should compile. BCL3 should work. We have some sharing in.
2. Test performance.
For 17 queries:
 - size of nonshared query
 - size of shared query
 - performance of nonshared query in yices
 - performance of shared query in yices
 - performance of nonshared query in seri
 - performance of shared query in seri

And come to some conclusions.
 


Thu Aug  2 09:52:24 EDT 2012

I don't know what the problem is with array. Let me just ignore it for now?

Look at the performance stuff. Perhaps I can avoid tracking down the problem
if I switch over to a heap evaluator entirely.

Thu Aug  2 10:06:36 EDT 2012

Started with performance:
shared:
  seri: 7 seconds without profiling
  yices: doesn't work because we can't redefine symbols :(
         that's bad. Maybe I can hack around it by just making up new names.
         That would be valuable.
  most time spent: a lot in free. So the elaborator is slow again. Only 38% in
  check now.
    
noshared:
  seri: 34 seconds without profiling
  yices: 11 seconds
  almost all time spent in check.
  
In other words... I think it's safe to say we want to do sharing. It speeds
things up by almost 5x here. And I bet, if we do the heap elaborator, not only
will performance of elaboration go way up, thus improving the shared case more
(because it's spending a lot of time in elaboration), also we should be able
to take advantage of that much more sharing.

Cool.

Not sure what to do about yices2 defining symbols with new names it shouldn't
be...

Unless... Can I push the definition of a symbol inside a query?

How about this, what if I collected a context of terms, which I insert into
things? No... that's not what I want...

Will the heap elaborator solve this issue? I don't think so. That's annoying.
How about an email to Bruno?

Thu Aug  2 10:32:10 EDT 2012

What I should do is manually rename the variables. That's not hard to do. Just
annoying. But I'll want it for the heap evaluator anyway. Keep a set of names,
and only rename it if needed. Write the rename traversal...

Thu Aug  2 10:41:38 EDT 2012

To do this more nicely, how about have a generic rename routine to rename free
variables? Oh, but that's not what alpha-renaming does. alpha-renaming is
different. Sigh.

Thu Aug  2 11:15:52 EDT 2012

Odd bug... For some reason, I'm using a type as an argument to a tuple update!
Yices, not surprisingly, is not happy with it. But why am I doing it?

Anyway, this renaming thing looks like it will work out just fine. I'm glad
about that. Also looks like yices does much better with the sharing, so we'll
definitely try to milk this possibility with the heap next.

But... what's going on here?

Somehow (Bool, Bool) is being treated as an object.

No, wait, that's not it. Unless it's a naming issue where tags can't have the
same names as types.

Thu Aug  2 11:23:53 EDT 2012

Same bug happens with Bluespec test, we just don't see it when we run seri for
some reason. Perhaps I should be doing better error checking?

Anyway, again it's a tuple thing...

I don't seem to have the same problem with TState...

Was this a problem with older versions of yices?

Apparently yes. What happened?

Thu Aug  2 12:05:29 EDT 2012

Okay, so I'm officially convinced now this is a yices2 bug. I sent out a
report. Until I hear back... I think I should have fun with the heap
elaborator.

Thu Aug  2 12:32:44 EDT 2012

Okay! Now for the fun stuff, eh? A heap elaborator.

Firstly, I should probably have some thoughts.

Can we make it cheap to do partial elaboration with the heap elaborator? Like
the kind of thing we do for running yices queries?

Idea is: take an Exp, translate to HExp, elaborate under the heap part way,
translate back to Exp, do a little work, then get a new expression and so on.

Can we be lazy in translating to the heap, so if we don't need to, we never
touch subexpressions? That would be nice.

One idea: have as a constructor of HExp, something which is just Exp. Don't
heapify it until we try to elaborate it.

So, this should work fine, unless we have free variables being shared in the
expression, right? If it's a LamE, and we are sharing the argument, doesn't
that mean we have to heapify the entire body?

Well, maybe it does. Maybe it doesn't. How about this, let me not worry about
it for now. Implement the heap elaborator without caring... see how well we
perform doing the translations back and forth... maybe I can be lazy if I want
though? That might be nice. See if there's a performance issue.

The first target use of the heap elaborator will be:
 serie - because we can
 yicese full elaborate - to get better sharing.

The last question is if it can replace what we have for run query. That's
certainly easy enough to try.

Good. The next question is, can we somehow keep things as scoped as possible?
I feel like we ought to be able to keep things fully scoped.

Maybe the way to make that work is to ... hmm...

The real question will be, how do we deheapify?

Let's say pointers are all STRefs. I have an expression, it has a bunch of
STRefs in it. How to deheapify?

One idea is: deheapify all the sub expressions, then return a let. The problem
is, we might loose sharing this way. We might try to deheapify the same sub
expression twice, which would be bad.

Okay, so here's an idea. Let's only deheapify those expressions which occur in
multiple different sub branches. Or rather, recursively traverse the entire
children expression to figure out if a reference happens in both children. If
so, deheapify that reference first. Any isolated to a single child can be
dealt with by the child.

It seems like we could do this with a two pass algorithm like thing.

First step: Ask children for their set of common sub expressions, combine
those together, and return it.

Now, at the top level we get a list of all the common sub expressions. We want
to push back ones that are not in common between the two... So, any common
between two children, you take ownership of.

That seems...messy. Like a lot of work. More work than I would like. But I
think it would give a very nicely cleaned and elaborated expression. Fully
inlined except where sharing could happen, which is nice.

Can I think of a better way to do this?

Going from the top down, the challenge is: given a set of children, figure out
all references referred to by both children.

Perhaps dynamic programming could be useful here?

No, I think what I first explained is the right way to go to start. Two
passes: first pass we collect at each node the entire set of reference by its
children. Second pass, given a list of references removed because they are
available at a higher level, find the intersection of children, make those
part of a let, remove them from children, recursively.

This could be a little annoying if I don't have a place to hold the
information about sharing along side the expressions for the final algorithm.
Perhaps I should add a parameter with each expression which is an annotation
for the node?

The other question is, can we somehow maintain this information as we
elaborate? That might be harder...

Fine. I'll go with this for now.

What is the high level algorithm for this then?

Let's assume I'm lazy in heapifying.
That means we want the following:

elaborate :: Mode -> Env -> Exp -> Exp

Which make make use of ST, and possibly state.
The state during elaboration will be the environment, and maybe also a mapping
from declared Sig to shared object, so we can have that kind of sharing too. I
like that.

If we have an Unheapified expression, we can just make one of those, then
elaborate the heapified expression.

How to elaborate a heapified expression? There are cases:

elaborateH :: HExp -> HeapM HExp

LitEH - do nothing.
CaseEH e ms - do a lazy pattern match: so try to match the first pattern. That
will elaborate as needed. If it succeeds, turn it into LamEH and elaborate
that. If it fails, dump the head, elaborate the rest. If it's unknown,
depending on the mode, either we are done now, or elaborate all the case
alternatives first as much as possible.
AppEH a b - elaborate a, see if it's a Lambda or primitive and act
appropriately.
LamEH s b - etc...

In other words, it's all just like the existing elaborator. There's just some
indirection to walk through.

How to do beta reduction? We have an arg, on the heap. If we totally heapify
in the first place, we could do all beta reduction at once during
heapification. If we want to be lazy, then wait to do beta reduction until we
have to, but its' all the same. Just substitute the pointer.
 
We could do explicitly lazy reduction. Associate with each subexpression a
mapping from variable name to STRef. That might be a little silly though.

I think I should not worry about being lazy yet. I should just fully heapify
ahead of time. Then beta reduction is done during heapification? Well, not
fully. What if we just have a lone Lambda? We can't do any reduction until we
know what the argument to it is.

So wait until we know the argument before doing any reduction, so beta reduce
at elaboration.

Good. Fine. Are we all set? Shall I dive in, see what I learn?

Oh wait. First I should read about STRefs.

I need a reference with the following property:
 - I can make a set of references
 - I can get a unique name from a reference.

Can you do that with STRef?

STRef has Equality defined. But looks like no name or hash or comparison.
Can I show an STRef? Probably bad to rely on it.

Looks like no. Can't make a set of STRefs.

Alternatively, I could give a name to each of my references. So a reference is
a name and an STRef. The name can be a number. I can use that for hashing and
comparison and naming. Easy solution.

Fine then. Let me start writing code. Maybe I can get a rough draft going by
the end of the day.

Thu Aug  2 13:24:16 EDT 2012

Question: full elaboration can lead to infinite data structures?

The only place for recursion is with declarations. If I use a hash table
mapping for that, then we can end up with cycles in the graph. If there are
cycles in the graph, I should try to figure out how to break them...

Let me not worry about cycles right now. Though I should pay attention to them
eventually. I'm going for a little bit at a time here.

Thu Aug  2 13:48:05 EDT 2012

I wrote down heapify. No trouble here, though I pushed much of the grunt work
into stubbed out helper functions.

How about implementing the core elaborator, elabH?

Thu Aug  2 14:28:04 EDT 2012

There's a question of what should be on the heap and what should be inline.
For example, we never need to store a literal on a heap, because it's as
simple as a reference.

The trouble with mixing them up, as I am now, is we have to explicitly
dereference everything after elaboration. This is slightly annoying.

Perhaps best would be to say: we always pass around references. We elaborate a
reference (it doesn't return anything, just makes sure the thing referred to
is elaborated as desired). Then I can have a dereference thing to look up the
value of a reference.

This makes it clearer I think. And easier, for example to implement
primitives: Once we find we have a primitive, elaborate the arguments fully,
dereference them, if we can do a reduction, do a reduction, otherwise do
nothing.

Okay, but you still would like an Exp -> Exp kind of thing, right? Otherwise
we have to explicitly update the results?

Let me think through how these things will work.
 LitEH ... well, we certainly want the derferenced thing to do the case match
 on, like I have now. We could return a Maybe, to say: don't update this.

 CaseEH: matches will update x as much as it cares to, that's good. If we do
 the match... then we change ourself? That makes sense to do.

 AppEH: VarE... we can change ourself.


Yes, better to make everything a reference. We'll want references to constants
anyway, because what starts as a complex expression could be reduced to a
constant.

Okay, change I'm going to make:
 - Every thing refers to references to expressions.
 - elaborate updates a reference in place (or not), it doesn't return
   anything.

Yes, I think that looks better now.

Thu Aug  2 15:03:27 EDT 2012

Hmm... how do we do beta reduction?

The problem is, the body could be shared across multiple applications. This
means, basically, we have to duplicate the body, right? But ideally we could
reuse common parts of the expression.

Well... it seems fairly clear what I should do. Have reduce return a maybe
reference. Nothing means there was nothing to do: use the original reference.
Something means there was a change. Propagate up the nothings as appropriate.

Thu Aug  2 15:34:11 EDT 2012

Beta reduction is pretty messy, but I wrote down the rough draft code for it.

Using a heap structure certainly makes the code... messier I would say.

Thu Aug  2 15:43:24 EDT 2012

Okay, so now the code for elaborate has been written down.

What things remain?
- deheapify
- declared
- little stub functions.

How about start with deheapify, because I think declared will just be tedious.
deheapify is much more interesting.

How will it work? I've already said the algorithm:
1. Get the set of each reference each expression has.
Note: references to VarEH, ConEH, and Lit shouldn't be included in the count.
So perhaps we should ask people to return their own reference too, or not, as
appropriate.

Where will I put these?
How about with each reference? Then I can ask each reference for it's set
of referred, and we can use a Maybe type just to make sure they set it right,
if you want. Or error initially... Well, Maybe is good, because we can see
things like: oh, this guy already did it. That is, we can leverage sharing
again, hurray!

So, we have a function to update the set of all children pointers for each
reference, self included. That's a simple traversal function.

Then, we have the function which, assuming this reference info is updated (or
not assuming, updating as it goes?) generates the lets. The input to the
function is the set of reference an expression doesn't have to worry about.

Fine. That's doable too. Let me start writing and see how far I get. I don't
think I'll have the rough draft done today, but I can get it pretty close.

Thu Aug  2 17:22:57 EDT 2012

I implemented reachable, it caches the results. It also doesn't include things
like references to VarE, ConE, LitE in the set.

Given that, deheapify should be real easy, no?

