
Tue Jul 31 09:04:34 EDT 2012

Plan for today: try to improve the >>= stats on the profile.

I think we're being overly lazy. So try to find that and get rid of it.

Plan:
1. Try making the Compilation strict in its fields, under the hypothesis that
we have a bunch of unevaluated thunks retaining lots of copies of this, which
is eating up memory, causing the allocator to have to do lots more work, which
is taking up a lot of time (and memory).

2. Read through the rest of the yices target, looking for weird stuff. In
particular, things like depat.

Wish me luck.

Tue Jul 31 09:13:09 EDT 2012

(1) didn't help any.

Tue Jul 31 09:18:40 EDT 2012

Looks like we are leaking Compilation objects, when I expect we shouldn't be.
We should only ever need just one. So let me focus on this.

First step, figure out who is retaining the Compilation objects.

The retainer profile says >>=, which really isn't so helpful. Let me
investigate by hand.

Things which have compilation objects:
 - SMTQuerier - we should check if this is being leaked.
 - call to runCompilation not evaluated? might hold onto ys.
    in both yicest and yicese.

In yices target:
 - modify may hold onto it if it's lazy.
    A strict modify would be useful.
 - gets call to get the commands (or anything) may hold onto it if gets is lazy.
    Perhaps a strict gets would be useful?

SMTQuerier is not being leaked. Good. It shows what we want to see the
Compilation look like: just one throughout.

Tue Jul 31 09:36:20 EDT 2012

Okay, so let me try everywhere to avoid leaking Compilation objects.

Tue Jul 31 09:49:35 EDT 2012

So all I could find to do was use getsS and modifyS everywhere.

Tue Jul 31 09:53:03 EDT 2012

Nope. Still leaking...

Tue Jul 31 10:00:19 EDT 2012

Got it! Not that it helped a whole lot, but at least it's clear >>= isn't a
major issue anymore.

But what was the change I made? I feel like I made a couple.

1. Use Control.Monad.State.Strict instead of Control.Monad.State.
2. Clear the cmds list before running instead of after running.

let me go back to Control.Monad.State. That is, let me start undoing changes I
made until I figure out the important difference.

Tue Jul 31 10:10:50 EDT 2012

You know what it looks like? It looks like we really didn't fix anything?
Hmm... I don't understand. The profile says we didn't improve anything... But
I swear we are leaking less, right?

But depending on how I implement addcmds, it either gets the cost of
performance and memory allocation, or >>= gets the cost.

Tue Jul 31 10:17:47 EDT 2012

Okay, so I definitely fixed the space leak, and using
Control.Monad.State.Strict has something to do with it, though I don't
entirely understand what's going on.

The profiling info, however, doesn't seem to reflect that. Perhaps it wasn't
costing us so much anyway. I don't know, but I think I should check this in
anyway.

Tue Jul 31 10:21:28 EDT 2012

Now what?

Our profile now has:
50% check
9% addcmds (35% alloc)
8% treplace (17% alloc)
8% ==

All at the top.

Tue Jul 31 10:23:12 EDT 2012

ytype is taking some amount of time. We could probably switch to using the
specific type interfaces for that instead of pretty printing. It's called a
lot, so it would be good to make it fast.

lookupDataConType could be made faster leveraging the hash table.

addcmds is getting lots of time an memory assigned to it. Why?

Monomorphic takes some time. I still feel like we should be able to reduce a
lot of work here by allowing incremental monomorphization. Perhaps it will
work better now that I've fixed this space leak, and I can make sure I don't
have the same space leak problem.

We could see if we are leaking MS from monomorphic. I don't see it as a huge
problem at this point.

Try to not use ytermbystr? It seems like pretty printing has more overhead
than I would expect.

In elaborate, beta reduction is the primary culprit. I feel like we could do
simultaneous reductions for case matches to speed things up.

Type inference takes a ton of time. Why is that?
It's mostly in the solver, in type replacement. Perhaps I can figure out a
better way to do that? Reduce the number of times it's called? Be lazy about
the replacement? Can I have smaller systems somehow?

For example, what if I did a pre-pass to simplify as many constraints as
possible, like the trivial constraints? Are there many of those?

So, we have a number of small things to look into. I think just do each at a
time, squeeze out more performance, in a cleanish kind of way if I can.

Tue Jul 31 10:43:24 EDT 2012

First goal: tackle addcmds.

What could the cost be coming from?
Well, 
 - ys_cmds forces ys?
 - lots and lots of repeated concatenation? (Which we don't need to do at all.

Oh... Look. We are appending commands.
That explains it. We have this whole big long list of commands, and we append
a couple to the back.

The answer? Use a reverse list. Prepend the commands. Then, always reverse
them when they are returned. Easy.

So this should be simple. Let me try it.

Tue Jul 31 10:53:52 EDT 2012

Yup! That was it. That helped a good deal.

What next?

Type inference is next after check on the profiling... It's worth looking into
this to see if there are any easy solutions.

I do think not using pretty in the yices2 interface should have a decent
impact too, even with type inference dominating so much...

Okay, so here's the plan. I'll look at the type inference thing. See if there
is anything obvious. If so, try to reduce it. If not, then move onto the ytype
and yterm thing: don't use pretty.

Tue Jul 31 10:58:50 EDT 2012

Okay, type inference. Here's the thing.

We come up with equations of the form: a = b
Then we make the replacement, anywhere you see a in the system and the
solution (both of which are big), replace it with b.

And we do this over and over and over and over and over again.

Is there some way to reduce the number of replacements we have to perform?
Could we group them together? Try to replace a lot at once? Use a hashtable to
perform the lookup efficiently and avoid equality checks?

What if we used a table and pointers?

Ah... there's an idea, no? Make use of sharing as much as possible.

So, instead of saying:

foo = bar sludge

Change everything with foo to bar sludge.
Now change everything with bar to baz.

We could say: 
Change everything with bar to baz.
Now change everything with foo to baz sludge.

Wouldn't that be much better?

Hmm...

We add all the constraints to the solution. Why do the replacement as we go?
Why not just wait until we have all the constraints, knowing they form some
sort of order, and make replacements after that? If we know the order, it may
even be automatically efficient in same way to do that.

I think we do want to do full substitution in the system as we go to solve it.
But we can at least start by cutting things in half.

Okay, this sounds good to me as something to try. Here's the idea:

- Don't perform replacement in the solution until the very end.

At the very end, we should have a mapping of the form
a: b

Where b only refers to elements earlier in the mapping, not later.

To finalize the solution set, we should do something like:

Oh, I should fix the above statement.

At some place in the solution, if we have:

a: b

That means that there is no reference to a above in the solution.

So, to finalize:

Replace all instances of b with a in all following formulas. Then finalize the
rest (after the substitutions). Simple. I'll try this out.

The other thing to look into is how efficient the replacement routine is.

One question is: do we ever have anything other than  VarT or a VarNT on the
left hand side? If so, no reason to do general equality. In fact, we could
just have the solution be from name to value. That would limit the scope of
replacements required... It's just like assign. And we could reuse assign.

Let me try that next, after this solution fix thing.

Tue Jul 31 11:20:06 EDT 2012

It works, but it doesn't make a significant difference in this case. Probably
most of the constraints are of the form a: Integer, or a: Bool, so we don't
save that much. I'll keep it in though.

Let me investigate more this idea of having the solution be a mapping from
String to Type? I'm not sure it will actually help all that much.

Is there some way we can take advantage of hash tables? If we can map from
name to type...

Let me focus on the finalization still, because that's still a big chunk of
the time being spent. We just do so many comparisons.

I think we could do better if we do some sharing.

Okay, so here's how it should work, I think.

Have the mapping be from Name to Type in the solution.

Now, to finalize, we use a fixed point like algorithm.

Repeatedly until we reach a fixed point (maybe haskell can do this for us
automatically using fix?):
1. Make a hash table out of the solution set.
2. Traverse the solution set, looking up each VarT or VarNT in the hash table
and making the replacement. 
This gives you a new solution set.

I don't think fix will work. But maybe we could be smarter.

Keep track of whether something was updated or not. If nothing was updated, we
are done?

Why is this a better solution than what I have?

Because we don't check N different things against each occurrence of a
variable. Instead each occurrence of a variable gets just a handful. However
many are needed to converge, and I expect we will converge rather quickly.

It would be nice if I could use fix to implement this... But I don't think we
can.

First step: switch to (Name, Type) map for the solution.

Tue Jul 31 11:51:37 EDT 2012

Hey, and once we do that, it's easy to reuse assign instead of rewriting our
own. I like that.

Not a significant performance change. We do use more memory now, I think
because assign takes a list instead of a single element.

Here's an idea: assign should take a hash table (it can convert the list into
a hash table to make it a transparent change).

Then, try to have lots of arguments.

In the solution: easy: do my fixed point thing. I bet that's cheaper than the
current thing. Just run until we get equality. That should be fine.

In the system we can do it too, maybe. Idea is... err... maybe not. Maybe I
should wait on that?

Well, what would be really nice is if we had a function: given the mapping,
and something assignable, continued to do the assignments until nothing
changed.

Now, this I can use for the solution and the system. The solution: just map it
over everything. I only need to create a single hash table. I bet it solves
pretty quickly. I don't think we have any really deep constraints.

For the system: just do this before calling single based on the existing
solution...

That sounds appealing to me. Let me try it after lunch.

So, remember the plan:

1. Make assign turn the list into a hash table before doing the assignments.
So, maybe have an assignh which is the class method, and a generic assign
function which wraps assignh in a call to making a hash table.

This could speed things up in its own right, hopefully it will.

2. Make my assign full thing, probably just need it for the type solver. Use
that for the solution and the system as described. Easy!

Let me have lunch first, then I'll try this out.

Tue Jul 31 13:05:17 EDT 2012

I changed Assign to use generics. Unhappily, that slowed things down. Also it
allocates a lot more memory. I think that's because we piggy back on top of
TransformM instead of implementing transform directly. It may make sense for
me to just implement transform directly for everything.

If generics are slow... we should figure out how to make them fast. Perhaps I
can have a preprocessor step to do it for me.

Let me try implementing Transform directly, and see if that solves this
slowdown I'm seeing. Otherwise... I don't know if I should keep the change of
the Assign implementation or not. I think it's much simpler, much cleaner,
much easier to maintain, avoiding a lot of redundancy... But it's slower. Is
that okay?

Tue Jul 31 13:23:42 EDT 2012

Well, it recovered some stuff, but not everything. Oh well. I'll keep it like
this for now.

Tue Jul 31 13:33:00 EDT 2012

Switching to a hashtable didn't really help so much. Perhaps I should just
give the option. Using generics, it's easy enough to have both options
available.

Then again, it didn't hurt too much either, did it?

Let me just have two implementations. assign, and assignh. Use assignh where
we have big things. Use assign where we have little. It will almost always be
little.

Yup, that helps.

Tue Jul 31 13:49:54 EDT 2012

And doing fixassign makes a big difference. Cool.

I wonder if there is a decent way to make it work on the system as well as the
solution? I was saying I could ... well... depends on how much work it is to
make a hash table. I suppose it's worth trying to use fixassign for the system
too. Just do it before substitution.

If that's too expensive, because we create too many hash tables, we could also
try using a non-hashtable version of that. Even better, I suppose, would be to
use a hashtable which supports fast inserting as well as updating.

Tue Jul 31 13:58:29 EDT 2012

Hmm... now we have a bit of overhead with creating the hash table. So what
should I do? Try assocs instead?

But why would this be any better? The advantage with the hash table is we have
lots of names looked up in parallel. The problem here is that my hash table
doesn't support incremental updates very efficiently. Creating lots and lots
of hash tables is expensive.

Tue Jul 31 14:02:39 EDT 2012

Here's a simple idea: use a Data.Map for the solution set. That should fix my
problem just swell.

Tue Jul 31 14:21:43 EDT 2012

Yup! That solves the type inference problem, I would say.

It may make sense, actually, to dump the hash table entirely for a hash map.
Let me try that.

Tue Jul 31 14:30:50 EDT 2012

Cool, type inference is now a solved problem, I would say.

64% check.
3% elaborate.
...

I think the next step is clear though. Avoid using ytermbystr if at all
possible. For both expressions and types.

Tue Jul 31 15:44:23 EDT 2012

We use ytermbystr for variables.
Hmm... So, these are not local variables. They are globally defined things.
The question is, how in the c api do I access these?

It's just yices term by name. Easy, no?

The other thing I'll want to change after this is constructing types.

Tue Jul 31 15:50:56 EDT 2012

ytermbystr works just swell. Cool. Now let me investigate types.

Hmm... profiling shows not using ytermbystr to be slower... though I think
really it is faster, it just doesn't know it. It just seems that allocating
the CString takes a bit of memory. That may just be the way things are.

Tue Jul 31 16:17:55 EDT 2012

I changed ytype to use the c api. Again, it uses less memory, and relatively
less time, but the profiler says things have slowed down... I don't know
what's up with that, but I think I'll keep the change for now.

Tue Jul 31 16:19:34 EDT 2012

Well, check is up to 75%. 
elaborate is back up there, at 2.6%, 5.4% allocation.

Basically, according to the profiling, check dominates, everything else is far
far behind. So I think I'm about where I want to be.

Myron's complaining about smt_diamond. Let me run profiling on that with bcl
to get an idea of what's wrong. Where is all the time being spent?

Tue Jul 31 16:36:34 EDT 2012

The profile for smt_diamond looks just like the profile for my BCL3 test.
That's good, it means I've been working on the right things.

Looks like yices is just slow?

Well... now I should really try an honest test. Which is see how long it takes
to run the yices2 query directly in yices. Compare that with my running time.

To get a first sense, let me try that (without profiling) on BCL3.

Tue Jul 31 16:39:56 EDT 2012

Without profiling, the seriq2 full thing takes: 33 seconds.

Tue Jul 31 16:51:30 EDT 2012

With debugging on... wow. This is one really really big query.

We'll see how long it takes to just dump it to a file. It doesn't surprise me
we spend a bunch of time in check, given how large the query is.

So, questions will be... how can we make the query smaller. For yices2, an
option is: Define constants as constants where-ever possible.

I could also take a look at the query itself in seri, and see if there is any
sharing or such that could be taken advantage of.

Tue Jul 31 16:57:55 EDT 2012

Using yices directly on the 300M query, it takes 13 seconds. So... 13 seconds
in raw yices, 33 seconds in seri. Is that reasonable?

Tue Jul 31 17:15:11 EDT 2012

So... what are we to do about how slow it is?

Oh yes, I remember. Let me take a look at the seri queries being asked, and
see if anything interesting shows up.

Tue Jul 31 17:22:11 EDT 2012

Well, we ask very similar questions, but they aren't the same. Things like:

aa == bb?
ab == ba?
ac == ca?
etc...

So I'm not sure how much sharing can be gotten from that.

One thing to try, just for the heck of it, is assuming no errors. See how that
affects performance.

What constants could I recognize? Would that help any for sharing? If we
defined a bunch of constants?

Hmm....

For example... bound things could turn into constants, right? Maybe?

But then, how do we apply a function to a constant? We can't really. I'm not
sure if that's what's responsible for all the time anyway.

Hmm...

Structural equality is another idea. Try to use that wherever possible?

Hmm... you know, we could have a deriving Eq thing, and we could derive an
equality that is a primitive structural equality that yices understands? Well,
not quite. That doesn't allow internal user defined equality.

Can I think of anything else?

Let me list all the things I can think of now, and I'll stew on it and get
back to you.

Ideas for why yices2 goes slower than Myron wants:
- Assume all cases will match something. (easy to try)
- Use structural equality instead of user defined equality.
- Common sub expressions should be defined as yices constants.
- In yices1, take advantage of functions.
- Use primitive and, not, or, etc... operations instead of user defined.
- Don't box bools.
- Use function-update primitive?
- Make sure the same questions are really being asked.
- Use let expression instead of over-beta reducing.

That's all I can think of for now. Some of these are pretty easy to try. It
would be worth giving them a try, even if they are ugly or hackish, just to
get a better sense of the performance issues.

