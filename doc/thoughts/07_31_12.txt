
Tue Jul 31 09:04:34 EDT 2012

Plan for today: try to improve the >>= stats on the profile.

I think we're being overly lazy. So try to find that and get rid of it.

Plan:
1. Try making the Compilation strict in its fields, under the hypothesis that
we have a bunch of unevaluated thunks retaining lots of copies of this, which
is eating up memory, causing the allocator to have to do lots more work, which
is taking up a lot of time (and memory).

2. Read through the rest of the yices target, looking for weird stuff. In
particular, things like depat.

Wish me luck.

Tue Jul 31 09:13:09 EDT 2012

(1) didn't help any.

Tue Jul 31 09:18:40 EDT 2012

Looks like we are leaking Compilation objects, when I expect we shouldn't be.
We should only ever need just one. So let me focus on this.

First step, figure out who is retaining the Compilation objects.

The retainer profile says >>=, which really isn't so helpful. Let me
investigate by hand.

Things which have compilation objects:
 - SMTQuerier - we should check if this is being leaked.
 - call to runCompilation not evaluated? might hold onto ys.
    in both yicest and yicese.

In yices target:
 - modify may hold onto it if it's lazy.
    A strict modify would be useful.
 - gets call to get the commands (or anything) may hold onto it if gets is lazy.
    Perhaps a strict gets would be useful?

SMTQuerier is not being leaked. Good. It shows what we want to see the
Compilation look like: just one throughout.

Tue Jul 31 09:36:20 EDT 2012

Okay, so let me try everywhere to avoid leaking Compilation objects.

Tue Jul 31 09:49:35 EDT 2012

So all I could find to do was use getsS and modifyS everywhere.

Tue Jul 31 09:53:03 EDT 2012

Nope. Still leaking...

Tue Jul 31 10:00:19 EDT 2012

Got it! Not that it helped a whole lot, but at least it's clear >>= isn't a
major issue anymore.

But what was the change I made? I feel like I made a couple.

1. Use Control.Monad.State.Strict instead of Control.Monad.State.
2. Clear the cmds list before running instead of after running.

let me go back to Control.Monad.State. That is, let me start undoing changes I
made until I figure out the important difference.

Tue Jul 31 10:10:50 EDT 2012

You know what it looks like? It looks like we really didn't fix anything?
Hmm... I don't understand. The profile says we didn't improve anything... But
I swear we are leaking less, right?

But depending on how I implement addcmds, it either gets the cost of
performance and memory allocation, or >>= gets the cost.

Tue Jul 31 10:17:47 EDT 2012

Okay, so I definitely fixed the space leak, and using
Control.Monad.State.Strict has something to do with it, though I don't
entirely understand what's going on.

The profiling info, however, doesn't seem to reflect that. Perhaps it wasn't
costing us so much anyway. I don't know, but I think I should check this in
anyway.

Tue Jul 31 10:21:28 EDT 2012

Now what?

Our profile now has:
50% check
9% addcmds (35% alloc)
8% treplace (17% alloc)
8% ==

All at the top.

Tue Jul 31 10:23:12 EDT 2012

ytype is taking some amount of time. We could probably switch to using the
specific type interfaces for that instead of pretty printing. It's called a
lot, so it would be good to make it fast.

lookupDataConType could be made faster leveraging the hash table.

addcmds is getting lots of time an memory assigned to it. Why?

Monomorphic takes some time. I still feel like we should be able to reduce a
lot of work here by allowing incremental monomorphization. Perhaps it will
work better now that I've fixed this space leak, and I can make sure I don't
have the same space leak problem.

We could see if we are leaking MS from monomorphic. I don't see it as a huge
problem at this point.

Try to not use ytermbystr? It seems like pretty printing has more overhead
than I would expect.

In elaborate, beta reduction is the primary culprit. I feel like we could do
simultaneous reductions for case matches to speed things up.

Type inference takes a ton of time. Why is that?
It's mostly in the solver, in type replacement. Perhaps I can figure out a
better way to do that? Reduce the number of times it's called? Be lazy about
the replacement? Can I have smaller systems somehow?

For example, what if I did a pre-pass to simplify as many constraints as
possible, like the trivial constraints? Are there many of those?

So, we have a number of small things to look into. I think just do each at a
time, squeeze out more performance, in a cleanish kind of way if I can.

Tue Jul 31 10:43:24 EDT 2012

First goal: tackle addcmds.

What could the cost be coming from?
Well, 
 - ys_cmds forces ys?
 - lots and lots of repeated concatenation? (Which we don't need to do at all.

Oh... Look. We are appending commands.
That explains it. We have this whole big long list of commands, and we append
a couple to the back.

The answer? Use a reverse list. Prepend the commands. Then, always reverse
them when they are returned. Easy.

So this should be simple. Let me try it.

Tue Jul 31 10:53:52 EDT 2012

Yup! That was it. That helped a good deal.

What next?

Type inference is next after check on the profiling... It's worth looking into
this to see if there are any easy solutions.

I do think not using pretty in the yices2 interface should have a decent
impact too, even with type inference dominating so much...

Okay, so here's the plan. I'll look at the type inference thing. See if there
is anything obvious. If so, try to reduce it. If not, then move onto the ytype
and yterm thing: don't use pretty.

Tue Jul 31 10:58:50 EDT 2012

Okay, type inference. Here's the thing.

We come up with equations of the form: a = b
Then we make the replacement, anywhere you see a in the system and the
solution (both of which are big), replace it with b.

And we do this over and over and over and over and over again.

Is there some way to reduce the number of replacements we have to perform?
Could we group them together? Try to replace a lot at once? Use a hashtable to
perform the lookup efficiently and avoid equality checks?

What if we used a table and pointers?

Ah... there's an idea, no? Make use of sharing as much as possible.

So, instead of saying:

foo = bar sludge

Change everything with foo to bar sludge.
Now change everything with bar to baz.

We could say: 
Change everything with bar to baz.
Now change everything with foo to baz sludge.

Wouldn't that be much better?

Hmm...

We add all the constraints to the solution. Why do the replacement as we go?
Why not just wait until we have all the constraints, knowing they form some
sort of order, and make replacements after that? If we know the order, it may
even be automatically efficient in same way to do that.

I think we do want to do full substitution in the system as we go to solve it.
But we can at least start by cutting things in half.

Okay, this sounds good to me as something to try. Here's the idea:

- Don't perform replacement in the solution until the very end.

At the very end, we should have a mapping of the form
a: b

Where b only refers to elements earlier in the mapping, not later.

To finalize the solution set, we should do something like:

Oh, I should fix the above statement.

At some place in the solution, if we have:

a: b

That means that there is no reference to a above in the solution.

So, to finalize:

Replace all instances of b with a in all following formulas. Then finalize the
rest (after the substitutions). Simple. I'll try this out.

The other thing to look into is how efficient the replacement routine is.

One question is: do we ever have anything other than  VarT or a VarNT on the
left hand side? If so, no reason to do general equality. In fact, we could
just have the solution be from name to value. That would limit the scope of
replacements required... It's just like assign. And we could reuse assign.

Let me try that next, after this solution fix thing.

Tue Jul 31 11:20:06 EDT 2012

It works, but it doesn't make a significant difference in this case. Probably
most of the constraints are of the form a: Integer, or a: Bool, so we don't
save that much. I'll keep it in though.

Let me investigate more this idea of having the solution be a mapping from
String to Type? I'm not sure it will actually help all that much.

Is there some way we can take advantage of hash tables? If we can map from
name to type...

Let me focus on the finalization still, because that's still a big chunk of
the time being spent. We just do so many comparisons.

I think we could do better if we do some sharing.

Okay, so here's how it should work, I think.

Have the mapping be from Name to Type in the solution.

Now, to finalize, we use a fixed point like algorithm.

Repeatedly until we reach a fixed point (maybe haskell can do this for us
automatically using fix?):
1. Make a hash table out of the solution set.
2. Traverse the solution set, looking up each VarT or VarNT in the hash table
and making the replacement. 
This gives you a new solution set.

I don't think fix will work. But maybe we could be smarter.

Keep track of whether something was updated or not. If nothing was updated, we
are done?

Why is this a better solution than what I have?

Because we don't check N different things against each occurrence of a
variable. Instead each occurrence of a variable gets just a handful. However
many are needed to converge, and I expect we will converge rather quickly.

It would be nice if I could use fix to implement this... But I don't think we
can.

First step: switch to (Name, Type) map for the solution.

Tue Jul 31 11:51:37 EDT 2012

Hey, and once we do that, it's easy to reuse assign instead of rewriting our
own. I like that.

Not a significant performance change. We do use more memory now, I think
because assign takes a list instead of a single element.

Here's an idea: assign should take a hash table (it can convert the list into
a hash table to make it a transparent change).

Then, try to have lots of arguments.

In the solution: easy: do my fixed point thing. I bet that's cheaper than the
current thing. Just run until we get equality. That should be fine.

In the system we can do it too, maybe. Idea is... err... maybe not. Maybe I
should wait on that?

Well, what would be really nice is if we had a function: given the mapping,
and something assignable, continued to do the assignments until nothing
changed.

Now, this I can use for the solution and the system. The solution: just map it
over everything. I only need to create a single hash table. I bet it solves
pretty quickly. I don't think we have any really deep constraints.

For the system: just do this before calling single based on the existing
solution...

That sounds appealing to me. Let me try it after lunch.

So, remember the plan:

1. Make assign turn the list into a hash table before doing the assignments.
So, maybe have an assignh which is the class method, and a generic assign
function which wraps assignh in a call to making a hash table.

This could speed things up in its own right, hopefully it will.

2. Make my assign full thing, probably just need it for the type solver. Use
that for the solution and the system as described. Easy!

Let me have lunch first, then I'll try this out.

Tue Jul 31 13:05:17 EDT 2012

I changed Assign to use generics. Unhappily, that slowed things down. Also it
allocates a lot more memory. I think that's because we piggy back on top of
TransformM instead of implementing transform directly. It may make sense for
me to just implement transform directly for everything.

If generics are slow... we should figure out how to make them fast. Perhaps I
can have a preprocessor step to do it for me.

Let me try implementing Transform directly, and see if that solves this
slowdown I'm seeing. Otherwise... I don't know if I should keep the change of
the Assign implementation or not. I think it's much simpler, much cleaner,
much easier to maintain, avoiding a lot of redundancy... But it's slower. Is
that okay?

Tue Jul 31 13:23:42 EDT 2012

Well, it recovered some stuff, but not everything. Oh well. I'll keep it like
this for now.

Tue Jul 31 13:33:00 EDT 2012

Switching to a hashtable didn't really help so much. Perhaps I should just
give the option. Using generics, it's easy enough to have both options
available.

Then again, it didn't hurt too much either, did it?

Let me just have two implementations. assign, and assignh. Use assignh where
we have big things. Use assign where we have little. It will almost always be
little.

Yup, that helps.
