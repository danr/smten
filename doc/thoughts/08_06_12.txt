
Mon Aug  6 10:06:29 EDT 2012

Plan for today:

1. Update version of yices, see if that bug was fixed.
2. finish implementing the free cache thing.
3. Fix whatever needs to be fixed for heap elaborator:
- should work on all tests (include array test) 
- should work when used as elaborator in SMT.Yices
- should work when used as full elaborator in yices target
- See what sorts of savings we get on performance, sharing for BCL3 with 17
  queries.

If that works, probably want to start on bit vectors, to try and get Jacob's
ATS dump running in seri (though I'm not sure what I can do with it...)

Here I go.

Mon Aug  6 10:19:00 EDT 2012

(1): bug was fixed. Good. Cool. One less thing to worry about. Now back to my
elaborator implementation.

Mon Aug  6 10:22:19 EDT 2012

Okay, I have the code for constructing the cache implemented... Now I just
need to add it to: 
 - argument to reduce
 - from there, argument to whoever calls reduce (LamEH)
 - from there, whoever constructs LamEH (heapify)
 - from there, whoever calls heapify: HeapifyEH.

And I think we're all set then.

Mon Aug  6 10:32:54 EDT 2012

Err... I'm worried.
Can we ever substute in something with a free variable?
I think we can, we we do full elaboration, in which case, we need to do alpha
renaming, don't we?

Ug. Let me worry about that when it comes up.

Mon Aug  6 11:45:24 EDT 2012

Nirav says the optimization isn't worth it. I should not attempt to share
between different beta reductions of the same function. Anyway, we simplify
the function before doing the beta reduction? Well... we don't go inside the
lambda to simplify it. Perhaps we should? I don't know. Erg.

Anyway, makes sense to make things work before worrying about this
optimization. Skip the reduction check, no need for the free cache. I'll see
how bad performance is after that.

But first, Nirav wants a bit vector library, with some examples working
through yices. Make bit vectors primitive. I should be able to do that fairly
reasonably. And also, shouldn't be hard to simplify and roll back the beta
reduction optimization.

Perhaps just use the simple optimization: if after reduction the result is
unchanged, don't return a new reference, just return the old one. Assume
HExpifies need reduction done to them. Or... I suppose, as a first step, I
could just call reducableE on those to check, and if that's too slow, then
change the implementation of reducableE to be true.

Cool. Sounds like a plan.

Mon Aug  6 12:16:43 EDT 2012

Note: we still don't pass the array update test or the array smt tests. They
get stuck in infinite loops. I'll look at those again later, after
implementing Bit vector support.

Anyway, good news is, without checking for reducable, it's much faster. Like,
an order of magnitude. So I'll leave it there for now, work on bit vectors (in
the original elaborator?) and then come back to this.

Mon Aug  6 12:21:04 EDT 2012

Oh, I should note, the problem with the Array smt test is not from the heap
elaborator, because we don't use the heap elaborator for it.

I could try switching to the heap elaborator entirely, but I have a feel it
won't work right for full elaboration because of the need for alpha renaming,
and I don't want to deal with that right now, so for now, leave it buggy? It
works on BCL3, which is what those people care about.

Mon Aug  6 12:22:34 EDT 2012

Fine, so... bit vector library. I can implement some tests and such. Get the
ball rolling.

First thing to try out?
- *, +, -, concat...

I don't know.

We can represent a bit vector in the elaborator as:
(fromInteger i), of the right type.

Let me come up with a test case. How about this, make a 3 bit vector,
increment it so it loops around, count how many increments that takes.

Also, I can have an SMT test for it: create a couple bit vectors, assert
something about their and and or, then see what we get for them.

Cool. Sounds like a plan to me. Let me get started.

Mon Aug  6 12:54:22 EDT 2012

Okay, so now I have to figure out how to implement bits in haskell. This will
be useful both for the haskell target library and for the elaborators.

First thing I need:
- a Bit data type, whose type includes the width.
- equality, add, sub, mul, fromInteger.

In the elaborator, we don't need a data type for it, we can just use
fromInteger. So perhaps what I want are just the raw bitvector operations,
done to integers.

And maybe they can take the width as input?

equality is just integer equality (assuming I always keep the integer properly
truncated).

add would be something like:

Bit.add :: Integer -> Integer -> Integer -> Integer

Where the first argument is the width of the addition.

The way it works? Um... how do we deal with negative numbers?

Shouldn't it just be: have a function that, given an integer, truncates it to
a given bit width. Deal with negative integers and such appropriately.

Then all the operations are: do the integer operation and truncate the result.

Sounds good to me.

Mon Aug  6 13:05:28 EDT 2012

I double checked. Doesn't look like there is already a suitable library in
haskell for bit vectors that supports arithmetic.

I should note, though, Integer is an instance of Bits...

I could define something which is an instance of bits. That may actually be
easier than always having to pass stuff around.

Okay, how about a non-statically typed bit vector library:

Bit = Bit { width :: Integer, value :: Integer }

I make it an instance of Bits, and we are all set. I can use that as the
target library and so on. Sounds good to me.

Mon Aug  6 13:27:26 EDT 2012

Okay, so I have my sample target library. Now I should be able to do this.

Mon Aug  6 13:49:48 EDT 2012

I added *, +, -, == to the elaborator. It's messy, but I think it works.

Next step: add it to the haskell target.

Mon Aug  6 14:20:01 EDT 2012

Okay, that seems to work. What's the next things to implement?

I guess, ideally, we should try it out in SMT.

I propose we try the following:

x <- free
assert (x + 1 == bit3 0)

And make sure it returns 7.

Mon Aug  6 14:27:39 EDT 2012

Ug. Trouble. How do we handle bit vectors in the monomorphizer? Considering
it's the kind of thing, for yices, we actually don't want to monomorphize...

Okay, turn it into "#n" for the integer n. That's fine with me.

Mon Aug  6 15:47:24 EDT 2012

Cool! So the basic stuff is there, the path for bit vectors through yices.
Honestly, I'd rather do the rest lazily. Hopefully that's okay.

Can I move on to the elaborator? Figure out what this bug is? I think I
should. At least for the rest of today. That will be good.

1. Figure out array bug.
2. Try using heap elaborator for everything, see how it works (I expect
failures in full reduction do to alpha renaming issues).

Oh, I need to also add bit vector support to the heap elaborator. Ug. Let me
do that tomorrow morning, because I don't feel like doing it now.

Mon Aug  6 16:09:35 EDT 2012

Trouble: it's going to be hard to debug this array thing. I can't step through
thousands of elaboration steps by hand to figure out what's going on.

I could try changing things randomly? Try being less lazy with reduce...

Or, don't let reduces pass each other. So, reduce of reduceEH should force the
reduceEH a step then reduce?

Mon Aug  6 16:16:14 EDT 2012

Well, I managed to break the loop by changing a reduceEH to a reduce. I'm not
sure if that totally defeats the purpose of wanting reduceEH to be lazy in the
first place.

Now we run into the full elaboration troubles.

We aren't doing full inlining for some reason.

Let me print out expressions before and after full inlining, just to see what
we get, and if it looks okay to me.

Mon Aug  6 16:35:50 EDT 2012

Trouble is with alpha renaming. We are capturing a variable we shouldn't be
capturing? Oh, no, that may not be the issue yet. No, there's something else
first. It's like, we just don't finish. We don't go into the second argument
of elaboration for (<) in the full mode.

Let me check out the code, see if that's expected.

Yup. I see the trouble here.

Mon Aug  6 16:45:18 EDT 2012

Looks like the Complex test gets stuck. Sadness.

And the simple query test is doing strange things I don't expect.

Question, why do I want beta reduction to be lazy?

If I do a beta reduction, we know I'm going to want to elaborate the whole
thing, so I may as well reduce the whole thing?

I suppose that's not entirely true. Say I have something like:

(\x -> foo x sludge) bar

I don't want to necessarily reduce sludge yet.

Whatever. Anyway, I should try to figure out what's up with Query1. Figure out
what I would like to see, what I expect to see, what I am seeing, and why.

Here's what I would like to see:

First assertion: 
    assert (free~1 < 6)

Second assertion:
    assert (free~1 > 4)

What I'm actually seeing:

Mon Aug  6 17:00:13 EDT 2012

First issue:

let x = free~1
in do
    assert (x < 6)
    assert (x > 4)
    query x

Is turned into:

let ~3 = free~1
in nobind_Query (let x = free~1 in assert (x < 6))
                (let x = free~1 in do
                    assert (x > 4)
                    query x)

There are a couple of issues with this.
1. Lazy reduction seems... odd here. Why do we want to do lazy reduction?
Oh, yes, I remember, it was a performance concern... Something to let us
lazily heapify. In particular, we want the argument to assert to be very
eazily deheapified?

Except... the argument, if unevaluated, is going to be so small anyway, the
cost of deheapifying is like, not much at all. So I think I shouldn't bother.

There is an easy fix here, which I think I should do: don't reduce lazily.

2. What's up with ~3? Why do we declare it and then not use it?

Let me start with don't reduce lazily. Because now that I think of it, that
seems silly. And... maybe even deheapify lazily is silly. Because, in
particular, deheapify doesn't do inlining like it did at one point. So
anything that isn't evaluated is going to be simple, and thus cheap to heapify
and deheapify.

What do you think? Dump lazy reduce and heapification?

Sounds like a plan to me. I can always add it back if we end up spending a lot
of time in deheapification.

So, what's the observation here? Besides that I should get more sleep at
night, because I do stupid things if I don't: premature optimization is a
waste.

Mon Aug  6 17:19:51 EDT 2012

Unfortunately, getting rid of deheapify and making reduce not lazy don't work.
We get stuck in an infinite loop.

I should try to go just a step at a time. First make reduce strict. Then
deheapify. Or, if that doesn't work, go the other way around.

Mon Aug  6 17:23:23 EDT 2012

Making reduce strict works fine. Let me continue along this line.

let x = free~1
in do
    assert (x < 6)
    assert (x > 4)
    query x

Is now turned into:

let ~3 = free~1
in nobind_Query (assert (free~1 < 6))
                (do
                    assert (free~1 > 4)
                    query free~1)

Good, that's better. But still we have this funny ~3 thing. What's with that?
Why is that being made into a let?

Let's look at deheapify to find out.

It must be considered a reachable reference...

Mon Aug  6 17:36:08 EDT 2012

It is considered reachable, because it's a HeapifyEH. We don't refer to it,
because ...

Because something is wrong...

When do I ever refer to a reference in scope?
I should do that when I call RefEH, not deheapify?

So this is a bug in deheapify.

Ah yes, I just never actually refer to the variables I define in let like I
should be. We build up this sharing and then ignore it.

In order to return the right thing, I need to keep track of the type of
expression a reference refers to. Or have some way to reconstruct it.

Okay, I'll do that first thing tomorrow or some such.

