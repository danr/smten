
Mon Aug 13 09:11:08 EDT 2012

Goal: Fix performance of the heap elaborator. Ideally in a systematic way.

Current snapshot of performance:

Looks like all the time not spent in check is spent in elaboration. Don't have
to worry about heapification or deheapification.

50% time, 80% allocation. Looks like we've got more space leaks to deal with.

elabHed has a lot amount of allocation and time just for it. That seems
strange to me.

mkRef has a fair amount of allocation just for it. That seems strange to me,
unless that's the expected allocation of the new reference.

match takes a bit of time. odd...

reduce has a lot of time and allocation.

Mon Aug 13 09:22:41 EDT 2012

Let's start with the biggest culprit: elabHed.

It's a terribly simple function.... That's odd.

Oh. It's probably being inlined. That would be annoying.

Anyway, let's see what type of things it is elaboratoring. Perhaps that will
give some insight into what it's leaking. We should do the same for mkRef,
which is leaking lots too.

Mon Aug 13 09:29:10 EDT 2012

elabHed... is allocating a bunch of ExpH. Not a ton. At most, like 1M's worth
for a query. I don't see thunks there. That could be normal?

We could look at what elabHed is retaining. That might be an interesting
question? Or... look at what all is being allocated in total?

Let me add some more cost centers, get a better idea of how things break down
in elabH.

Mon Aug 13 09:51:05 EDT 2012

Looks like most of the allocation comes from the CASE branch of elaboration.

mkRef is allocating mostly ExpR. That might actually be expected, just because
we are creating so many references to expressions. It is called, like, 2.5
million times in the worst usage. And this is by reduce.

Here's an idea: when reducing for case statements, don't convert to lambda
first, do all the reductions simultaneously. That could cut the reductions in
half.

And it looks like all the mkRef trouble is from calls to reduce.
 
Mon Aug 13 10:00:34 EDT 2012

Hmm... looks like the problem with CASE is in UNMATCHED mostly?

Now readRef1 shows up the most... Let's see what it's allocating.

Mon Aug 13 10:12:22 EDT 2012

Okay, so I think I'm making some progress.

If you look at the type of memory allocated by the elaborator, almost all of
it is from ExpRs. ER, ExpR, Type, Integer, MUT_VAR_CLEAN, STRef.

So, let's see who is retaining ExpRs?

Is there a way to figure out if we have the same number of ExpRs as ER?

Well, it looks close. Let's see who's retaining ExpRs first. Then see who is
retaining ERs. Try to clean those up if possible.

Mon Aug 13 10:17:14 EDT 2012

Retaining ERs is: reduce, mkRef. But mostly reduce. And reduce.rm.

I should be able to get rid of this leak. We should, ideally, not be retaining
anything.

Mon Aug 13 10:23:16 EDT 2012

I don't understand. For a strict state monad combined with ST... if I say
something like:

y <- foo x
return y

Is it possible for that to leak 'x'? Or does it force the value of y?

Perhaps I should experiment...

Mon Aug 13 10:31:52 EDT 2012

What I want is, I think, every FooEH object should hold an ExpR directly, not
a thunk. And then... every ExpR should hold a ExpH directly, not a thunk. If
we can make that happen, then we should be in decent shape I think.

How can I make that happen? I feel like ! on data type fields should help, but
every time I've tried that, it hasn't made any difference. Well, let me try
again, see if it does anything.

Mon Aug 13 10:42:10 EDT 2012

Nope. Just made things worse. Hum.

I don't know what's being made into thunks or why. I fear that mkRef, and
reduce, and all that is done entirely lazily, when I want it to be done
entirely strictly. How can I force that? How can I make the result of reduce
be fully evaluated?

What if I had a deepSeq for my heap. You give it a reference, it returns the
reference fully, deeply, elaborated. Can I do that? Would that help anything?

So, ST is not strict in values. That means when we say readSTRef or
writeSTRef, we could be reading or writing a thunk. But... we can't hide
something like reduce on the thunk, can we? Because it uses ST. It could
create things in ST?

Mon Aug 13 11:35:50 EDT 2012

Nirav pointed out some funniness in the queries I'm generating. Lots of silly
duplications. I should figure out what those are. I might have better luck
with that than this lazy issue... Who knows. Let me take a look at it
anyway, just to get a feel for what the issue is.

Query things I'm seeing:

let x = blah in x

This is silly. Where does this come from?

Looks like:
  case free of
    (x, _) -> x

Well, I see why that's leading to the generated code it is.  It's a yices
specific thing I think. We could recognize it specially if we wanted. I'm not
sure it will make so much of a difference. I suppose it's worth a try.

But it's not the problem with the heap elaborator. Just something else to keep
in mind for later.

What to do about the memory leaks in the heap elaborator?

Let me focus on the specific problem: REDUCE_APP

REDUCE_APP is retaining ExpRs.

do
 a' <- reduce s v a
 b' <- reduce s v b
 mkRef (rtype r) $ AppEH a' b'

What leaks here?
 - a leaks if reduction hasn't happened yet.
 - b leaks, if reduction hasn't happened yet.
 - a', b' don't leak, because we need them. But! It could be marked as leaking
   if we don't ... No. They shouldn't leak because they are on the AppEH,
right?
 - We leak r! In the call to (rtype r). That's bad.

How can I force that?

We return (mkRef ... ...) as a thunk. That's also an issue. Who figures that
out?

Okay, so someone wants the result of reduce. To figure it out, we have to read
r and figure out the type is AppEH. But then we just return a thunk. No need
even to evaluate the do block, we leak a, b, r.

So, I have to look at who uses the result of reduce, and make sure they force
it to at least weak head normal form. Even then, though, we leak r through the
type. We leak a, b, perhaps, through the expression.

Users of reduce:
 LamEH - we very quickly reelabH it, which should force the ExpR.
 reduce.rm - we don't force the result.
 reduce - recursive calls, as we know, don't force anything.

And that's it.

Okay, so let me try this, as an experiment:
1. Make a function which, given a reference, returns the reference after
having completely, deeply, elaborated it. So, read all the references, case on
them. Also make sure the types and ids are seqed. Then call this after the
LamEH call to reduce. Do this, and see what it does, if anything.

Mon Aug 13 12:40:15 EDT 2012

Well, it looks like it did something. Takes longer, more memory allocated, but
less leaking... Lovely.

Gah! Ug.

Let me try to fix the yices thing to make the query less silly, print that
out, then look to see if there is more stupidity in the generated query.

I should also recognize nested lets.

Mon Aug 13 12:57:11 EDT 2012

Nested lets didn't work out, because I guess the bindings don't take effect
until the body of the let.

But the other change: let x = blah in x, now written as blah, cuts down the
query from 74M to 66M. That's good.

What else do I see?

- We don't need to define a new case variable if the argument to the case is
simple, such as free~1.

- Make the WHNF elaborator be sharing. Or have that option so different
  assertions can make use of sharing. I bet that helps a bunch.

- inline bools... but again, I don't think this is the most important thing at
  this point. I should be working on the elaborator.

Mon Aug 13 13:37:55 EDT 2012

Okay, let me be more structured in how I do this.

1. Looks like the simplified yices query doesn't work for debugging. Figure
out when I broke that and fix it. Check in the changes to the yices target.

2. Reduce multiple things at a time for case reduction. That should improve
performance significantly.

3. Go back to profiling, brainstorm how to improve performance.

Mon Aug 13 13:58:10 EDT 2012

The bug was switching to SWHNF. I wonder why...

Anyway, that was the bug there.

Now, SWHNF is a good potential future thing. For now, let me move on to (2).

Mon Aug 13 14:09:23 EDT 2012

Actually, better yet, let me skip to (3).

Review of profile:

readRef1 has lots of time and memory.
APP has lots of time, some memory.
mkRef has lots of time, tons of memory.
== has lots of time (no memory).
UNMATCH has lots of time, lots of memory.
elabH has lots of time, tons of memory.

And so on.

Mon Aug 13 14:17:34 EDT 2012

30% of the time is in reduce. That's impressive.

That also would justify sharing reduce for multiple bindings in case
reduction.

Question: why does mkRef take 4.3% time, 12% allocation? What is it
allocating? Let me break it down into parts to see.

Mon Aug 13 14:27:01 EDT 2012

Okay, so, we are allocating all the references: ExpR, STRef. Not surprising it
takes up memory.

Mon Aug 13 14:45:40 EDT 2012

What if we do beta reduction a little differently.

For each Lambda, represent it as:

LamEH ExpR ExpR

The first ExpR is a reference to the variable, and the body has already done
the reduction. Now, for beta reduction, all you have to do is update the
variable reference? Yes. That should work. As long as the LamEH isn't
shared... Oh. Yes. So if the LamEH is shared, then that runs into problems.
What we really want is (App (Lam ...) ...). That's where the substitution is
unique. Bugger. Fundamentally we have to copy the lambda body for each
different application.

Well, regardless, doing all the case reductions at the same time should help,
so let me try that, see how much difference it makes. Just for the sake of
some improvement.

Mon Aug 13 15:04:58 EDT 2012

Well, certainly it's improved, so I see no reason not to commit this change.

Committed. Now, reduce still takes up 13% time, 32% allocation still. All of
this is from straight beta reduction, not from case pattern matching
reduction.

Does anything generate an application of a lambda? I feel like the answer is
no. The only thing that does is inlining of a variable from the environment?

Well, or things like:

let f = \a -> ...
in f x + f y

Bummer. But... what if I can make it more likely? Then I could ... I don't
know. The idea is, you really want to duplicate as little as possible when
doing beta application. If a function is only applied once, then only make
one version of it in memory, don't make a copy of it.

Another idea is to do beta reduction lazily. The value of this is, we can say:

reduce m (ReduceE m' e) = reduce (m++m') e

So we are combining our lookups, and reducing the number of traversals we
make, and reducing the number of expressions we duplicate for the same reason.
That sounds valuable to me.

How hard is this? Elaborating ReduceEH is just calling reduce, which does a
single level of reduction, and elaborating the result. Deheapification can
also apply the reduce. Err... it's slightly annoying though. Unless elaborate
fully gets rid of the lazy reduce, but then where is the laziness used?

Mon Aug 13 15:35:14 EDT 2012

Summarizing again the profiling:

APP: 40/66
CASE: 13/21
UNMATCHED: 6.5/9
matches: 4.0/7.7
reduce: 13/33
elabHed 6.1/7.4

Brainstorm:
- space leaks?
- flatten case statements to make matching easier?
- lazy beta reduction to avoid needless copies of things?
- cache free variables to avoid needless reduction?
- avoid copy for beta reduction with a single use of lambda?
- do beta reduction when possible at heapify time?
- Higher order abstract syntax representation for lambdas?

Hmm... higher order abstract syntax representation for lambdas made at
heapify?

So, LamEH holds (\ExpR -> ExpR). You give an arg, it gives the result. Beta
application is just function application.

How can I build this up? heapify has to return a function... or... make it a
lazy heapification?

heapify :: [(Name, ExpR)] -> Exp -> ExpR

Then just do: LamEH (\r -> heapify n r e)

What's the value here? We don't duplicate anything that doesn't need to be
duplicated? It's like a lazy reduction/heapification...

Hmm... And, I can always inline it easily enough...

But how do you elaborate the body? Stuff it into the lambda I suppose.

\r -> elaborate (f r)

What will this representation give over what I do currently? We lazily
construct the heap version of the expression, and we reconstruct it for each
different application. So I avoid an extra copy of the expression. But that's
really all I can think of.

You know what? I think that would actually help. It would certainly be fun to
try, see what issues come up, and see how much of a difference it makes. I'm
basically getting rid of the reduce entirely, which should make us 10% faster
for free? At least it does lazy reduction for free, but unfortunately, we
still stack reductions on top of each other, right?


Mon Aug 13 17:24:04 EDT 2012

Here's another idea. What if I kept things in sharing form, as in, with let
expressions directly. Like, in my nheap elaborator?

So, to elaborate a beta reduction... don't do anything! We just keep it in
shared form. But, err... that is, we keep it like that, then go into the body.
Elaborate the body with a scope which points the name to its value, and inline
that when we get there. The one thing we do need to do is simplifications
where you have: let x = blah in foo, where foo doesn't make any reference to
x, should be just foo.

As an added bonus, this leaves us with the lets we want, all in the right
place.

I suppose my real question is, do I think the heap elaborator will be faster
than the non-heap elaborator? Or in any way better? And the answer now, is,
currently? I don't think so. Too complicated. Too many reference allocated...

Another question... can I use HOAS in the non-heap elaborator? Have a modified
kind of expression, just like the original, only with LamE represented as a
function? Well... that's kind of what my idea of having a scope is, right?

Let me take a look at the non-heap elaborator, review where it is slow.

Mon Aug 13 17:33:41 EDT 2012

Yes, the non-heap elaborator is in much better shape, and has room for
improvement. Most of it's time is spent in alpha renaming, reduce,
should-reduce...

Let's say I did a separate uniquification pass. Then I wouldn't have to worry
about alpha renaming...

Let's say I don't actually perform any reduction in the sharing form. Then I
don't have to worry about alpha renaming, do I? The idea is, elaboration takes
place in a scope, so I'm elaborating...

Let me use my tried and true alpha renaming example:


\b -> (\a -> (\b -> (a + b))) b

I go inside... but now I have a scope:

b: VarE b
a: VarE b
b: VarE b

a turns into: "b", but it turns into the wrong "b", unfortunately.
Except, I could do alpha renaming as soon as I see that "b" is in scope?

inscope: b

Except, as I've run into many times before, we don't know who all is in scope.
I should just do a separate renaming step, and not worry about this?


How about my sharing test.

quadruple a = a + (a + (a + a))

Elaborate: quadruple (x-y)

(\a -> a + (a + (a + a))) (x-y)

let a = (x-y)
in a + (a + (a + a))

But, if a should be reduced, then we want to do the reduction.

Mon Aug 13 17:45:27 EDT 2012

Wait a second, we already only do beta reduction if there are no free
variables in the argument. If that's the case, then we don't have to worry
about alpha renaming, we can't capture anything.

Let me try that out.

Oh, not quite. Because if the argument is a function type, we are going to
reduce it anyway.

Or again, don't do the reduction, just add something in scope? Perform
evaluation with a scope...

I feel like this should speed things up a bunch. Reduction becomes trivial:
constant time. Do we still need renaming? I would love it if we didn't...
Well, if we are going to do uniqification, we could do it at the same time.

elaborate with a scope... doesn't help make use of sharing though. Not unless
we elaborate the argument in the scope?

And doesn't that become our heap structure, just like that? Isn't that a
better kind of heap elaborator?

So, I propose a new heap elaborator. Elaboration happens in an environment
mapping variable name to expression?

Or... better yet, do a translation into a representation which is just like
the IR, only for Lambdas use HOAS. That avoids the need for alpha renaming,
right? Let haskell do all the work. Beta reduction is function application.
It's all pure... Do we maintain sharing? How do we elaborate a lambda, for
example, that's in HOAS form?

Oh, easy, stick the elaboration into the lambda function.

So, reduction becomes trivial. We basically are doing all the reduction at the
conversion.

I don't know. It may not work out the way I want.


let's try it out on our old alpha renaming friend at least:

\b -> (\a -> (\b -> (a + b))) b

Names are unique, because there are no names? I can make up my own names when
I go to print it? When I go to convert back?



