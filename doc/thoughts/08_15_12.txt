
Wed Aug 15 07:56:17 EDT 2012

Goal for today: fix alpharenaming/reduction issue in elaborator/delambdafier.

Starting in the delambdafier, I think, because it's much simpler, but still
runs into all the interesting issues.

Also, consider HOAS. Can I make use of it somehow?

The idea behind HOAS is, I think, exactly what I'm trying to do. Beta
reduction should just be a function call... in fact, that's what I did in the
delambdafier, but alpha renaming was more of an issue there. It may have more
success in the elaborator.

So, that's something to try, but first I would like to take a stab at alpha
renaming in the delambdafier. If I can figure that out in a nice way, then I
think I'm set for the elaborator too (if we find that needs renaming).

I know the concern with alpha renaming. Whenever an expression moves inside a
lambda (because we pushed it inside the lambda, or because of beta reduction),
there is a change the lambda captures a free variable in that expression that
it shouldn't.

I have a proposal from before. Let me put that up as a straw man.

The idea is: don't allow shadowed variables. Don't allow duplicates in scope?

Erm... I should try an example. And one that we may find in delambdafy.

Cases in delambdafy:

1. beta reduction of a function type with free variables which could be
  captured
2. pushing application inside a lambda. We know the one possible name that
could be captured, if that helps any. 
3. pushing application inside a case. We know the set of possible names that
could be captured, from the pattern bindings.

The idea is: do we know what free names are in scope?

There are cases to consider.

1. The free name is declared.
This isn't a problem in delambdafication, because we assume full inlining? At
least, we do the way I use it. But in general, this is something we would have
to worry about.

2. The free name is from going inside a lambda.
Then we know about the name.

3. The free name was made up from outside. So it's not in the environment, and
we don't know about it.
This is annoying. We could perhaps get around it by requiring the user to tell
us about it. This is kind of like case (1).

I feel like I want to solve just (2), but in reality, to do things right, we
should handle all (1, 2, 3).

This means, the only way I can know what needs to be renamed is by doing a
traversal throughout the expression. We could do that up front as a single
pass perhaps.

That's an interesting idea. It goes like this: do a preliminary pass.

1. alpha rename the entire expression, getting rid of free names. Now all
lambdas are unique, and can't capture anything?

Well, that's only if you do the renaming in such a way as to make it unique.

But this is certainly doable. The question is, will it gain us anything? Is it
worth the whole big renaming, exploring the entire expression, just to avoid
renaming in intermediate places?

Well, here's something: that uniqification could be shared by elaboration and
delambdafy. I bet that makes sense to do. It could also be shared by both
heaps...

Wait. Can we share it in the elaborator? Not until after we've inlined,
otherwise we don't know what all things are free or not, right?

One solution to that would be: uniqify every expression in the environment
(lazily). So, we could have a way to uniqify an environment. A way to uniqify
an expression. But that won't work, because inlining causes sharing again
which could lead to name clashes. We have to uniqifiy lazily.

Can we locally pick new names for things? If so, I'm tempted to just always
pick a new name for every lambda that we go by?

ug.


Wed Aug 15 08:21:24 EDT 2012

With my reduction thing, time spent in delambdify is mostly from alpha
renaming.

I could make a free cache. Would that help?

It would help in elaborate to detect free variables.

It would ... not really help with alpharenaming so much.


Okay, new idea. Let me see if I can convince myself it will work.

We have a uniqify function: [Name] -> Exp -> Exp. Given an expression, renames
all lambdas to something not in the given list of free variables.

This uniqify is local, not global. This means two lambdas down a different
path may be given the same name. Is that a problem?

AppE (\s -> blah) (\s -> blah)

No, because the bindings aren't free. You will never capture a free variable,
because the lambda has a name different from any free variable.

AppE (\x -> \s -> x+s) (\s -> blah)

\s -> (\s -> blah) s

Oh. Here's an example where that may not work.

AppE (\x -> \s -> x (s+1)) (\s -> add 1)

\s -> (\s -> add 1) (s+1)

For delambdification, we push the argument s inside:

\s -> (\s -> add 1 (s+1))

Where we have just captured the s. Sadness.

How much does it hurt us if we use "free" instead of what's in scope in
delambdafy? That's more appropriate. Perhaps a better thing to do is improve
the performance of alpharename?

Okay, here's the plan.

1. Try using "free" instead of tracking free variables. It will have to do a
lot more traversal, which is sad, but it's more correct, and may reduce the
amount of renaming we have to do, and then we don't have to keep track of
free, which should help, I think.

Wed Aug 15 08:55:57 EDT 2012

Idea to make free faster: use a Set instead of a list of names. That should
help nub, right?

Also, make bindingsP' better.

Wed Aug 15 09:15:44 EDT 2012

It made things worse to use a Set instead of a list. Bummer.

Wed Aug 15 09:23:28 EDT 2012

Well, I don't know. I could try doing the reduce thing in elaborate. Maybe
that will help. That's like a HOAS kind of thing. And if we divorce it from
alpha renaming, it should work fine, functionally speaking.

The trouble is, reduce doesn't actually cost us that much. Most of the time is
spent in alpha renaming.

Wed Aug 15 10:58:17 EDT 2012

Okay! So I have two proposals which I believe are promising.

1. Combine alpharename and reduce in elaborate.
Relatively simple change. We always do alpharename and reduce together anyway.
This will hopefully save us a traversal.

Change reduce to do any necessary renaming. Pass it as an extra argument the
free variables in the argument being reduced.

Reduce stays the same aside from the following: any time we are about to enter
a lambda, if the lambda has the same name as a free variable, pick a new name
for it, and start reducing that name to.

Reduce should now work on multiple arguments again. This could be useful for
case statements later on, or not. I don't think it matters much.

I think this sounds very promising to me. The cost of alpharenaming hopefully
mostly goes away, the lowish cost of reduction hopefully stays low, and we
save a bunch of traversals and such.

2. HOAS.
Represent LamE as a function from Exp to Exp.

This is more complicated than proposal (1), but could pay off. How it works:

1. Translate Exp to this new representation. Do it at the start, and any time
we inline a variable.

The function will be: \x -> reduce n x b, where n is the bound name, and b is
the body of the lambda.

2. Do elaboration as normal. Don't worry about beta reduction. Reduce is just
application of the lambda. HOAS will maybe save us something here? It's hard
for me to imagine how, but at least we skip alpha renaming.

3. Translate representation back to Exp. This is where we have to deal with
alpha renaming. We choose the names for the lambdas that are still around. We
should just choose fresh, new names. Names that aren't free. So, do a
traversal, get the list of free names, then pick new names based on that.

The value of this approach: we only have to do alpha renaming once.

I don't honestly think the HOAS part will save us much, but that's fine. We
can at least do really cheap renaming when represented as HOAS. At least, I
think it's cheap? Oh, I suppose not. It's just the same as reduction. I'll
think about it.

Wed Aug 15 12:50:02 EDT 2012

Some new thoughts.

- The HOAS stuff can be made to work with combined reduction/alpha renaming
  just fine I think. I should try that first if reduction/alpha renaming join
  helps.

- Be more specific about what I want from things.
  For example, instead of: n `elem` free e, do something like hasfree n e. 

  That way I avoid making a list of things.

Cool. Let me go to work then. I'm feeling hopeful.

Wed Aug 15 13:06:40 EDT 2012

I'm going to try joining reduction and alpha renaming now.

Let me review how it should work.

1. Write a hasfree function to work on expressions.
2. reduction need not take a list of free. Instead, whenever it gets to a
lambda which doesn't shadow the reduction, check if the argument being reduced 
has the free variable. If so, rename the variable, and start reducing that
too.

Trivial. Simple. Easy.

3. for shouldreduce, write a function which is: has non-prim free. Use that
instead of building up a list of all the free variables.

I'm hopeful. Very hopeful...

Wed Aug 15 13:27:57 EDT 2012

Some trouble.

Firstly: I have to make sure my renamed things don't themselves get captured.
Secondly: Even after making the substition, I have more to do...

This suggests to me that reduce should really be a multi-reduce thing. And
just check for empty reduction as a special case.

Wed Aug 15 13:58:26 EDT 2012

Ug. So, this has turned out to be a little messier than I hoped. We'll see if
it leads to any performance improvement. Also keep in mind, I may want to try
caching the free names in expressions if we end up calling hasfree too many
times.

Wed Aug 15 14:06:39 EDT 2012

There! I think I've got it now. Ug. That's a bit of work. I would be rather
surprised if it works at all.

Anyway, there we have reduction, let me switch to using it, see if it works,
and see how performance goes.

Wed Aug 15 14:19:19 EDT 2012

Well, it appears to work, and I think the code is cleaner, or at least,
shorter, because we only do a single traversal. But it didn't really make any
performance difference...

Wed Aug 15 14:29:09 EDT 2012

Here's an idea: have two versions of reduce. One which does renaming, one
which doesn't. And the one which does, it can do only single renaming if
needed: because the only time we do beta reduction with a free variable is if
it is a VarE! I'll try that next.

Wed Aug 15 14:36:31 EDT 2012

Oh, but that's wrong, because we still need to rename to avoid capture of
primitives? Except, I'm almost positive you can't pick a name for our
primitives, because they all have a "." in them. So that shouldn't be a
problem. That's cool. Let me try this.

Wed Aug 15 14:42:19 EDT 2012

Cool. Good. I think, perhaps, alpha renaming and reduction are no longer the
top concerns. Let me look at the profile now, see what I see.

check: 9.6, it's on the top now, which is good.
elaborate': 8.9, where is this from? Pattern matching?
>>=: 5.7, what does this mean?
==: 4.7, where is this from?

lookupDataConType is fairly bad, I should take a look at that again, make sure
it uses hash tables as appropriate.

yExp.dematch
yCon looks expensive.

elaborate': 24 total. That's big.
reducern: 2.4
assign: 4.1, that's pretty big.
lookupVar: 3.5: uses lookupInstD, make sure that's done efficiently in the
environment.

hasNonPrimFree: 3.4
 Much of which is isprim: so, lookupVarInfo.

delambdafy: 12.5
  Mostly alpha renaming.
  Not surprising. Perhaps I can make that better, like I did for elaborate.
  free' also.

Monomorphic: 16! That's a lot. I should look into this more closely. Space
leaks perhaps? Did I already check for those?

runCmds:  11.90
Wow. That's a lot of time there.
 

So, the long and short of it is: beta reduction and alpha renaming is no
longer a prime target. We are seeing a big smattering of things which could be
contributing here and there to the time. That's good, I think. It means I've
been making improvements.

The cost of == comes from finish in monomorphic finish. I bet I can improve
that a bit. And some in 'free' used in type checking.

Good. So I think I might leave elaboration alone for the time being.

Wed Aug 15 15:03:42 EDT 2012

Let's take a look at that monomorphic == and see if we can't reduce that cost
any.

I know one thing about monomorphizing, is I don't have it incremental, when I
could, which you would think should save me a bunch of repeated work. When I
tried making it incremental, however, it slowed things way down. I think
because of problems with strictness. Let me work on making it faster as is,
get it tuned, then, if it's still a problem, try making it incremental.

Wed Aug 15 15:05:25 EDT 2012

The == must be coming from nub. I can make a smarter nub, right?

Also, I'm probably leaking monomorphic state. It would be worth fixing that
now.

Wed Aug 15 15:07:56 EDT 2012

Hmm... the == isn't doing stupid things. It may make sense to switch to using
a set? If things don't need to be ordered.

First, though, strictness stuff. Let's see who leaks MS.

Looks like monotype and monoexp.

Wed Aug 15 15:13:34 EDT 2012

Good. Fixed that.

Wed Aug 15 15:17:47 EDT 2012

Looks like monomorphic doesn't need order. Let me try using sets instead of
lists. Because we're doing lots of tests for equality.

Wed Aug 15 15:25:42 EDT 2012

Looks like a reasonable, but small, improvement. I'll keep it.

Wed Aug 15 15:42:14 EDT 2012

Okay, how do I figure out where all the elaborate time is being spent?

Wed Aug 15 16:04:07 EDT 2012

I did a little test which is: remove the bitvector primitives from the
elaborator and check performance. It does make some difference it turns out,
to have big long lists. So... maybe I can encapsulate it in a function? Maybe
we want a hash table or something to look up the primitives?

I'm not sure what I can do about this.

Wed Aug 15 16:10:43 EDT 2012

Looks like even something as simple as changing the order of the case
statement makes a difference. Perhaps I should use nested cases.

