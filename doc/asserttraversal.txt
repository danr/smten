Thu Jan 30 17:03:18 EST 2014

Assert traversal takes up a significant amount of time in many smten
applications. The goal of this discussion is to fix that.

The problem is the time it takes to lookup and insert expressions in the
caches. Ideally we could have a mini, one-element cache for each expression,
and this time would be totally eliminated.

First step: identify a benchmark which exhibits the assert traversal issue
particularly well:
  nqueens -s yices2 -e Int 24
  Takes 2 seconds. 80% time in assert traversal from insertions and lookups.

Next step: identify which caches are taking all the hits.

As expected: the Bool cache.
  Cache Lookups: 580K
  Cache Misses: 373K
  Cache Hits: 207K

I tried a LinearHashTable just for the fun of it. It goes much slower.

Break down Cache lookups and misses by constructor:

And: ~178K
Not: ~178K

It's split between And and Not.

Ideas to evaluate:
* Get unsafeIO with local caches working.
  Start by adding a local cache to all the Not constructor, get that to work.
  Steps:
   * Generate a "unique" for each assert traversal, carry it around in the
     context.
   * Define an AssertCache in AssertCache.hs
     With methods:
     new :: IO AssertCache
     lookup :: (Typeable a) => AssertCache -> Unique -> IO (Maybe a)
     insert :: (Typeable a) => AssertCache -> Unique -> a -> IO ()
   * Add a AssertCache field to NotFF, construct NotFF using notFF
     which does unsafePerformIO. See if it works or not. Good luck.
* Use weak pointers and remove GC'd elements?
  Would that do anything?
  The documentation for it looks very sketchy. I don't have high hopes about
  this approach.

Fri Jan 31 09:00:08 EST 2014

I tried splitting the bool cache into two: one for NotFF, and the other for
other kinds of boolean formulas. It doubled the runtime! That, to me, is not
at all expected.

Why could it be? Maybe:
* Now you are actually forcing the bool object to get the cache?
* Two small caches have to be resized just a little bit more than one 
big cache?

I don't think it's the first, because the case should be forcing the object,
and we force it whenever we do the cache lookup anyway.

Does profiling give a hint at the second?

Profiling says:
 * The Other cache spends significantly more time deleting and lookup up
   elements, even though the number of deletes and lookups are the same.

Could this be due to machine cache behavior? The BasicHashMap deals in cache
lines, and be switching between the NotFF cache and the AndFF cache, we 
mess up caching behavior?

Isn't there a way to see cache hits and cache misses in linux?

Running /usr/bin/time with all the options doesn't show anything interesting.

Fri Jan 31 10:03:43 EST 2014

I tried implementing the per-formula assert cache for the Not constructor.
It appears to work!

nqueens runtime dropped from 2 seconds to 1.4 seconds.
The time to lookup the cached value of the bool dropped from 35% to 15%.
And memory improved as well.

Cool.

The next step will be applying this approach to all of the formulas.

Then... we'll see how much of the problem is caching and how much of the
problem is assert traversal, and we'll see if this has any bad implications
for overall performance when the assert traversal wasn't a bottleneck.

I switched all formulas over to per-formula caches.

After all that:
 nqueens dropped 1.96 to 1.57.
 Assert dropped from 80% time to 74% time. 

Not nearly as good as I was hoping. I think there is some bottleneck or silly
performance block that perhaps we can do away with.

Ideas:
* We should specialize and inline everything in Assert
  - Because the Supported class is just to for metaprogramming
* Make AssertCache.cached inlineable?
* I didn't put caches on all the objects
  - Maybe we should be caching literals and vars too?
* Use dupable unsafe perform IO?
* Reader monad overhead: maybe pass ctx and key as arguments everywhere
  instead of using the reader monad.

Let me look at profiling to try and understand. In particular, the difference
between just and and not, and having everything per-formula cached.

All the difference is inside the assert traversal. Let me add profiling to
that.

* 40% is in uservar.
* 20% is in cached.

Small, but perhaps non-trivial amount of time is spent in unary, binary, and
trinary.

Note that enabling profiling and adding SCCs also has a notable impact on
performance.

Let me try the following:
1. Specialize and inline everything in assert traversal.

First: do we already specialize and inline everything?

In particular, what I want to see is no "Supported" dictionary anywhere in the
generated code:

* unary, binary, and trinary should all be specialized
* calls to 'build' should all be to the specialized versions

What we actually see currently:
 * 'build' takes a Supported dictionary
 * 'uservar' has table ops totally inlined
 * $fSupportedBitFF1 is 'build' for BitFF
   - binary is inlined and specialized

In other words, it looks like Supported is totally inlined away. Perfect.

2. Make 'cached' inlineable, because currently that is not specialized. In
particular, I want to specialize it for the Reader monad and see if that
helps.

Answer: no, it does not help. It doesn't really make a difference, but if
anything, it makes things worse. So I'll undo that change.

3. Try caching vars, because I think that could be the big thing.

Yup! That was it.
Total time for nqueens is down to under 1s.

The assert traversal is down to 50% time. Memory is still up at 70% allocation
(down from 80%).

Note, though, I'm not sure I can believe the profiling numbers because they
don't include the optimization that I verified is happening inlining
supported.

That's disconcerting...

What if I try turning on optimization with profiling?
It makes no difference to the reported numbers.

Good. I think I have the assert traversal fixed up as desired.

Now the real question is, how does the performance of other applications hold
up? Is this an overall improvement?

The answer: it appears overwhelmingly yes, this is a performance improvement
every application I tried.

So we keep it and call it done. Cool.

